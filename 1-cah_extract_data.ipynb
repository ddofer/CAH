{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ae1e0b",
   "metadata": {},
   "source": [
    "### Extract data & train/test sets \n",
    "* pretrain data: all black + which + combinations\n",
    "    * Could supplemenet with more black/white from internet, and other jokes sources (reddit, jester) but overkill. \n",
    "    * Main goal is just to \"warm up\" a language/sentence model, generically. \n",
    "\n",
    "* train/test sets: Split by game/`fake_round_id`\n",
    "    * Train data: Drop duplicates\n",
    "    * keep \"all\" 1 cases, and keep all # occurrences by default\n",
    "\n",
    "* IGNORE pick 2 cases for now (overcomplicates evaluation) ; but keep them for the pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1e26b",
   "metadata": {},
   "source": [
    "Each row of the file represents a white card that was given to a player in a round and how it performed. It also contains information about the round like how long it took and what black card was dealt.\n",
    "So columns that describe general round data will be exactly the same for all ten rows of that round. These rows are: fake_round_id, round_completion_seconds, round_skipped, black_card_text, and black_card_pick_num.\n",
    "But columns that describe a property of the white card will change over the ten rows of that round. These rows are: white_card_text, won, and winning_index.\n",
    "\n",
    "\n",
    "The columns:\n",
    "* fake_round_id: a unique integer for every round. This is to help you separate one round from another to find which 10 cards were presented in every round.\n",
    "* round_completion_seconds: the number of seconds it took for the user to pick a winner after being presented the cards for that round. You'll probably want to filter out users that decide too quickly by setting some minimum threshold for this. Some of these numbers can be insanely high as well since the user just left their browser open for hours or possibly days before picking a winner.\n",
    "* round_skipped: we have a red button in the top right of the lab page that says \"No Good Plays!\". If a user clicks this button, round_skipped will be \"True\" and no white card will be marked as a winner.\n",
    "* black_card_text: self explanatory.\n",
    "* black_card_pick_num: The number of white cards that the black card requires. Generally 1, but will be 2 for black cards like \"That's right, I killed _____. How, you ask? _____.\"\n",
    "* white_card_text: self explanatory.\n",
    "* won: True if this white card won the round.\n",
    "* winning_index: This is for storing the order that the winning card(s) were picked in. If the white card didn't win, this will just contain a null value (NULL). If the card won and the black card was a normal \"Pick 1\" this will always be 0. But if the black card was a \"Pick 2\", one of the winners will be 0 and the other will be 1. This is how you figure out which blank in pick 2 black cards was meant to be filled by which winning white card. The first blank is filled by the 0 and the second one by the 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e49f8",
   "metadata": {},
   "source": [
    "* sample groups (for test);\n",
    "\n",
    "* https://stackoverflow.com/questions/44007496/random-sampling-with-pandas-data-frame-disjoint-groups\n",
    "    ```\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "    # Initialize the GroupShuffleSplit.\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.5)\n",
    "\n",
    "    # Get the indexers for the split.\n",
    "    idx1, idx2 = next(gss.split(df, groups=df.ids))\n",
    "\n",
    "    # Get the split DataFrames.\n",
    "    df1, df2 = df.iloc[idx1], df.iloc[idx2]\n",
    "    ```\n",
    "\n",
    "* https://hippocampus-garden.com/pandas_group_sample/\n",
    "    ```\n",
    "    sampled_users = np.random.choice(df[\"user_id\"].unique(), 100)\n",
    "    df_sampled = df.groupby('user_id').filter(lambda x: x[\"user_id\"].values[0] in sampled_users)\n",
    "    df_sampled[\"user_id\"].nunique()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "experienced-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option('display.width', 1024)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "%matplotlib inline \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score  \n",
    "# https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef51172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util ## versions error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0a6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CSV = False#True#False ## load raw data from csv instead of parquet (which has had some cols dropped)\n",
    "MIN_PAIR_FREQ = 1\n",
    "SAVE_TRAIN_DATA = False#False#True\n",
    "\n",
    "LOWERCASE_DATA = False\n",
    "DROP_DOUBLES = True ## drop white cards with pick 2 options ; and their blacks. (TODO: analyze interesting pairs)\n",
    "DROP_SKIPPED = True ## ignore rounds where skipped\n",
    "\n",
    "DATA_PATH = #REDACTED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-paris",
   "metadata": {},
   "source": [
    "Load data\n",
    "\n",
    "* Note there are some near-duplicate cards (e.g. footballers - could minhash - https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.py , LSH/shingles : https://onestopdataanalysis.com/lsh/  (broken library?) , https://github.com/zyocum/dedup/blob/master/dedup.py ,\n",
    "\n",
    "`Datasketch` https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/ ,\n",
    "https://stackoverflow.com/questions/25114338/approximate-string-matching-using-lsh , \n",
    "\n",
    "SKLearn friendly?: http://ethen8181.github.io/machine-learning/recsys/content_based/lsh_text.html\n",
    "\n",
    "`dedupe` library - https://github.com/dedupeio/dedupe\n",
    "\n",
    "* Paraphrase mining using sentence bert - https://www.sbert.net/examples/applications/paraphrase-mining/README.html  (may be too semantic)\n",
    "\n",
    "* Example: \"10 football players with erections *barreling* towards you at full speed\" - 10 football players with erections **barrelling** towards you at full speed.\"\n",
    "\n",
    "Task modelling:\n",
    "* Ranking = most accurate\n",
    "* Regression/classification: can be done just over single cards, or between pairs. \n",
    "    * if pairs-wise: can use sentencebert/SNLI models (which assume pairs of sentences) - https://github.com/UKPLab/sentence-transformers\n",
    "    \n",
    "    \n",
    "Examples:\n",
    "\n",
    "* https://www.sbert.net/examples/training/sts/README.html#training-data\n",
    "* https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli.py\n",
    "* https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py\n",
    "\n",
    "\n",
    "* Keras example (not sentenceBert): https://keras.io/examples/nlp/semantic_similarity_with_bert/\n",
    "\n",
    "\n",
    "* Interpretability tool : https://github.com/cdpierse/transformers-interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sound-transcript",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.91 s\n",
      "Wall time: 1.32 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>round_completion_seconds</th>\n",
       "      <th>round_skipped</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>black_card_pick_num</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>ID_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Going inside at some point because of the mosquitoes.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Being fat from noodles.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Letting this loser eat me out.</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>That chicken from Popeyes.®</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>A sorry excuse for a father.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989545</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! _____ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Breastfeeding a ten-year-old.</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989546</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! _____ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy daddies with happy sandals.</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989547</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! _____ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Jerking off to a 10-second RealMedia clip.</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989548</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! _____ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting naked and watching Nickelodeon.</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989549</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! _____ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Awesome pictures of planets and stuff.</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2446360 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fake_round_id  round_completion_seconds  round_skipped                                                                               black_card_text  black_card_pick_num                                        white_card_text  won  ID_index\n",
       "0                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.                    1  Going inside at some point because of the mosquitoes.    0         1\n",
       "1                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.                    1                                Being fat from noodles.    0         2\n",
       "2                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.                    1                         Letting this loser eat me out.    0         3\n",
       "3                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.                    1                            That chicken from Popeyes.®    0         4\n",
       "4                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into _____, and I love to have a good time.                    1                           A sorry excuse for a father.    0         5\n",
       "...                ...                       ...            ...                                                                                           ...                  ...                                                    ...  ...       ...\n",
       "2989545         298955                      7613          False                                                                Oh my god! _____ killed Kenny!                    1                          Breastfeeding a ten-year-old.    0         6\n",
       "2989546         298955                      7613          False                                                                Oh my god! _____ killed Kenny!                    1                      Happy daddies with happy sandals.    0         7\n",
       "2989547         298955                      7613          False                                                                Oh my god! _____ killed Kenny!                    1             Jerking off to a 10-second RealMedia clip.    0         8\n",
       "2989548         298955                      7613          False                                                                Oh my god! _____ killed Kenny!                    1                Getting naked and watching Nickelodeon.    0         9\n",
       "2989549         298955                      7613          False                                                                Oh my god! _____ killed Kenny!                    1                 Awesome pictures of planets and stuff.    0        10\n",
       "\n",
       "[2446360 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#### Original processing of data, before I saved it as a parquet for faster subsequent loading: \n",
    "if LOAD_CSV:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    ## remove punct/newlines\n",
    "    df[\"white_card_text\"] = df[\"white_card_text\"].replace(\"\\n\", \" \\n \",regex=True).replace(\"\\t\", \" \\t \",regex=True) # .replace(\"\\r\", \"\")\n",
    "    df[\"black_card_text\"] = df[\"black_card_text\"].replace(\"\\n\", \" \\n \",regex=True).replace(\"\\t\", \" \\t \",regex=True) # .replace(\"\\r\", \"\")\n",
    "\n",
    "    ## add number within group - may not actually be the read order shown on screen!! \n",
    "    df['ID_index'] = df.groupby('fake_round_id').cumcount() + 1\n",
    "\n",
    "    print(df[['black_card_text', 'black_card_pick_num', 'white_card_text',\"fake_round_id\"]].nunique())\n",
    "    print(df.describe())\n",
    "    \n",
    "    ## save to disk in compressed, binary format\n",
    "    df.drop([\"winning_index\"],axis=1).to_parquet('cah_lab_data.parquet')\n",
    "\n",
    "else:  # load data saved in above code previously\n",
    "    df = pd.read_parquet('cah_lab_data.parquet')\n",
    "#     df= pd.read_feather('cah_lab_data.feather')\n",
    "\n",
    "if DROP_DOUBLES:\n",
    "    df = df.loc[df[\"black_card_pick_num\"]==1]\n",
    "if DROP_SKIPPED:\n",
    "    df = df.loc[~df[\"round_skipped\"]]\n",
    "    \n",
    "# added:\n",
    "df[\"won\"] = df[\"won\"].astype(int)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298c9f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>round_completion_seconds</th>\n",
       "      <th>black_card_pick_num</th>\n",
       "      <th>won</th>\n",
       "      <th>ID_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.4464e+06</td>\n",
       "      <td>2.4464e+06</td>\n",
       "      <td>2446360.0</td>\n",
       "      <td>2446360.0</td>\n",
       "      <td>2.4464e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.4917e+05</td>\n",
       "      <td>8.9592e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.5000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.6119e+04</td>\n",
       "      <td>3.1008e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.8723e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0000e+00</td>\n",
       "      <td>2.0000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.4948e+04</td>\n",
       "      <td>1.0000e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.4858e+05</td>\n",
       "      <td>1.7000e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.2351e+05</td>\n",
       "      <td>2.7000e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.9896e+05</td>\n",
       "      <td>7.0210e+05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fake_round_id  round_completion_seconds  black_card_pick_num        won    ID_index\n",
       "count     2.4464e+06                2.4464e+06            2446360.0  2446360.0  2.4464e+06\n",
       "mean      1.4917e+05                8.9592e+01                  1.0        0.1  5.5000e+00\n",
       "std       8.6119e+04                3.1008e+03                  0.0        0.3  2.8723e+00\n",
       "min       1.0000e+00                2.0000e+00                  1.0        0.0  1.0000e+00\n",
       "25%       7.4948e+04                1.0000e+01                  1.0        0.0  3.0000e+00\n",
       "50%       1.4858e+05                1.7000e+01                  1.0        0.0  5.5000e+00\n",
       "75%       2.2351e+05                2.7000e+01                  1.0        0.0  8.0000e+00\n",
       "max       2.9896e+05                7.0210e+05                  1.0        1.0  1.0000e+01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ff658",
   "metadata": {},
   "source": [
    "* Weak, but clear not extreme bias in favor of \"first\" cards, or in middle of screen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa63063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake_round_id               1.1587e-19\n",
       "round_completion_seconds    1.6692e-19\n",
       "round_skipped                      NaN\n",
       "black_card_pick_num                NaN\n",
       "won                         1.0000e+00\n",
       "ID_index                   -1.2425e-02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corrwith(df[\"won\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33dd17de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_index\n",
       "1     0.1035\n",
       "2     0.1077\n",
       "3     0.1042\n",
       "4     0.1027\n",
       "5     0.0995\n",
       "6     0.0987\n",
       "7     0.0962\n",
       "8     0.0957\n",
       "9     0.0949\n",
       "10    0.0967\n",
       "Name: won, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ID_index\"])[\"won\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2080567",
   "metadata": {},
   "source": [
    "#### normalize \\____ 's \n",
    "* Rememebr to do this if evaluating\n",
    "* May not be needed!\n",
    "    * We also do this for consistency when doing find and replace , and for cases where there's no ___ in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"has_\"] = df[\"black_card_text\"].str.contains(\"_{2,}\")\n",
    "### filter out cases with 2 blanks to fill ? \n",
    "\n",
    "df['black_card_text'] = df['black_card_text'].str.replace(\"_{2,}\",\"__\")\n",
    "## https://stackoverflow.com/questions/47696401/replace-character-of-column-value-with-string-from-another-column-in-pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea306b22",
   "metadata": {},
   "source": [
    "### Pick 2s\n",
    "* Keep only subset for free text, and self join is slow.... \n",
    "* Keep only sample of combinations, as otherwise we get huuuge # combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4314572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"black_card_pick_num\"].describe() ## max 2, min 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9078dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doubles = df.loc[df[\"black_card_pick_num\"]>1].filter(['black_card_text', 'white_card_text',\"won\",\"fake_round_id\"],axis=1).drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c70205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"black_card_pick_num\"]<2] ## keep non doubles only\n",
    "\n",
    "# df[[\"id_black\",\"id_white\"]] = ord_enc.fit_transform(df[[\"black_card_text\",\"white_card_text\"]])\n",
    "df[\"id_white\"] = ord_enc.fit_transform(df[\"white_card_text\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ee1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DROP_DOUBLES:\n",
    "    df_doubles[\"black_card_text\"] = df_doubles[\"black_card_text\"].str.replace(\"(PICK 2)\",\"\",case=False,regex=False)\n",
    "\n",
    "    df_doubles[\"black_card_text\"] = df_doubles[\"black_card_text\"].str.replace(\"__.\",\"__\",case=False,regex=False)\n",
    "    df_doubles.sort_values(['black_card_text',\"won\"],inplace=True,ascending=False)\n",
    "    # display(df_doubles)\n",
    "    print(df_doubles.nunique())\n",
    "    print(df_doubles.drop_duplicates(['black_card_text', 'white_card_text']).shape[0],\"unique double combinations\")\n",
    "    print(\"Rows:\",df_doubles.shape[0])\n",
    "\n",
    "    ### biased sample - winning combinations + some others (otherwise, we have too many possibles..)\n",
    "    df_doubles_1 = df_doubles.loc[df_doubles[\"won\"]==True]\n",
    "    df_doubles_2 = df_doubles.loc[df_doubles[\"won\"]==False].groupby([\"black_card_text\",\"fake_round_id\"]).sample(2)\n",
    "    df_doubles = pd.concat([df_doubles_1,df_doubles_2]).drop_duplicates(['black_card_text', 'white_card_text'])\n",
    "    print(\"after biased sample: Rows:\",df_doubles.shape[0])\n",
    "    df_doubles.set_index([\"black_card_text\",\"fake_round_id\"],inplace=True)\n",
    "\n",
    "    # ## keep first, last occ per all the white cards - biased sample, \n",
    "    # df_doubles = pd.concat([df_doubles.drop_duplicates(keep=\"first\"),\n",
    "    #                         df_doubles.drop_duplicates(subset=[\"white_card_text\"], keep=\"last\")\n",
    "    #                        ]).drop_duplicates()\n",
    "    # df_doubles = df_doubles.drop_duplicates(subset=[\"white_card_text\"],keep=\"first\")\n",
    "\n",
    "    ### could filter for mirror images ? \n",
    "    ### https://stackoverflow.com/questions/24676705/pandas-drop-duplicates-if-reverse-is-present-between-two-columns\n",
    "    # df['check_string'] = df.apply(lambda row: ''.join(sorted([row['InteractorA'], row['InteractorB']])), axis=1)\n",
    "\n",
    "    # df_doubles.drop(columns=\"won\",errors=\"ignore\",inplace=True)\n",
    "    print(\"Rows (after dropping):\",df_doubles.shape[0])\n",
    "    df_doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b13448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not DROP_DOUBLES:\n",
    "    ### slow merge + check string\n",
    "    # df_doubles = df_doubles.merge(df_doubles,on=[\"black_card_text\"])\n",
    "    ## self join\n",
    "    df_doubles = df_doubles.join(df_doubles[\"white_card_text\"],lsuffix=\"_x\",rsuffix=\"_y\")\n",
    "    df_doubles = df_doubles.loc[df_doubles[\"white_card_text_x\"]!=df_doubles[\"white_card_text_y\"]]\n",
    "    # ## drop mirror images (maybe relevant, but too redundant)\n",
    "    df_doubles['check_string'] = df_doubles.apply(lambda row: ''.join(sorted([row['white_card_text_x'], row['white_card_text_y']])), axis=1)\n",
    "\n",
    "    df_doubles = df_doubles.reset_index().drop_duplicates(subset=[\"check_string\"])\n",
    "    df_doubles =df_doubles.sort_values(['black_card_text',\"won\"],ascending=False).drop(columns=[\"won\",\"check_string\",\"fake_round_id\"],errors=\"ignore\").drop_duplicates()\n",
    "    print(df_doubles.shape) ## 200 million by default\n",
    "\n",
    "\n",
    "    %%time\n",
    "    ## remove . at end of text\n",
    "    df_doubles[\"white_card_text_x\"] = df_doubles[\"white_card_text_x\"].str.replace(\"\\.$\", '',regex=True)\n",
    "    df_doubles['text'] = df_doubles.apply(lambda x:x['black_card_text'].replace(\"__\", x['white_card_text_x'],1), axis=1)\n",
    "    df_doubles['text'] = df_doubles.apply(lambda x:x['text'].replace(\"__\", x['white_card_text_y'],1), axis=1)\n",
    "    df_doubles['text'] = df_doubles['text'].str.replace(\"..\",\".\",regex=False) ## fix double dots. Still levaes extra punct.. \n",
    "    # df_doubles = df_doubles[[\"black_card_text\",'text']].drop_duplicates()\n",
    "    print(df_doubles.shape)\n",
    "    df_doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709907b7",
   "metadata": {},
   "source": [
    "## df with all texts from data\n",
    "white, black and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f09106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.concat([df[\"black_card_text\"],\n",
    "                       df[\"white_card_text\"],\n",
    "#                     df_doubles['text'],df_doubles['black_card_text']\n",
    "                    ]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc04e2",
   "metadata": {},
   "source": [
    "Fill in \"punchlines\" into blanks (___) (or add at end of text for cards without ___)\n",
    "\n",
    "* NOTE: Previously I had tried a model with black and white text as 2 prompts (which meant the model needed to learn to fill in blanks); and a model of just the filled in answer (1 col). \n",
    "* Unclear what is \"easier\" to learn/model\n",
    "\n",
    "* Note this doesn't handle cases with double blanks (it will just input the same punchline twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb471154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>round_completion_seconds</th>\n",
       "      <th>round_skipped</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>black_card_pick_num</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>ID_index</th>\n",
       "      <th>has_</th>\n",
       "      <th>id_white</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Going inside at some point because of the mosquitoes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>927.0</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into Going inside at some point because of the mosquitoes, and I love to have a good time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Being fat from noodles</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>428.0</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into Being fat from noodles, and I love to have a good time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>Letting this loser eat me out</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into Letting this loser eat me out, and I love to have a good time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>That chicken from Popeyes.®</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into That chicken from Popeyes.®, and I love to have a good time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>1</td>\n",
       "      <td>A sorry excuse for a father</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>231.0</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into A sorry excuse for a father, and I love to have a good time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989545</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! __ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Breastfeeding a ten-year-old</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>495.0</td>\n",
       "      <td>Oh my god! Breastfeeding a ten-year-old killed Kenny!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989546</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! __ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy daddies with happy sandals</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>957.0</td>\n",
       "      <td>Oh my god! Happy daddies with happy sandals killed Kenny!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989547</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! __ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Jerking off to a 10-second RealMedia clip</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>Oh my god! Jerking off to a 10-second RealMedia clip killed Kenny!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989548</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! __ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting naked and watching Nickelodeon</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>889.0</td>\n",
       "      <td>Oh my god! Getting naked and watching Nickelodeon killed Kenny!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989549</th>\n",
       "      <td>298955</td>\n",
       "      <td>7613</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my god! __ killed Kenny!</td>\n",
       "      <td>1</td>\n",
       "      <td>Awesome pictures of planets and stuff</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>388.0</td>\n",
       "      <td>Oh my god! Awesome pictures of planets and stuff killed Kenny!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2446360 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fake_round_id  round_completion_seconds  round_skipped                                                                            black_card_text  black_card_pick_num                                       white_card_text  won  ID_index  has_  id_white                                                                                                                                         text\n",
       "0                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                    1  Going inside at some point because of the mosquitoes    0         1  True     927.0  Hi MTV! My name is Kendra, I live in Malibu, I'm into Going inside at some point because of the mosquitoes, and I love to have a good time.\n",
       "1                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                    1                                Being fat from noodles    0         2  True     428.0                                Hi MTV! My name is Kendra, I live in Malibu, I'm into Being fat from noodles, and I love to have a good time.\n",
       "2                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                    1                         Letting this loser eat me out    0         3  True    1134.0                         Hi MTV! My name is Kendra, I live in Malibu, I'm into Letting this loser eat me out, and I love to have a good time.\n",
       "3                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                    1                           That chicken from Popeyes.®    0         4  True    1719.0                           Hi MTV! My name is Kendra, I live in Malibu, I'm into That chicken from Popeyes.®, and I love to have a good time.\n",
       "4                    1                        24          False  Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                    1                           A sorry excuse for a father    0         5  True     231.0                           Hi MTV! My name is Kendra, I live in Malibu, I'm into A sorry excuse for a father, and I love to have a good time.\n",
       "...                ...                       ...            ...                                                                                        ...                  ...                                                   ...  ...       ...   ...       ...                                                                                                                                          ...\n",
       "2989545         298955                      7613          False                                                                Oh my god! __ killed Kenny!                    1                          Breastfeeding a ten-year-old    0         6  True     495.0                                                                                        Oh my god! Breastfeeding a ten-year-old killed Kenny!\n",
       "2989546         298955                      7613          False                                                                Oh my god! __ killed Kenny!                    1                      Happy daddies with happy sandals    0         7  True     957.0                                                                                    Oh my god! Happy daddies with happy sandals killed Kenny!\n",
       "2989547         298955                      7613          False                                                                Oh my god! __ killed Kenny!                    1             Jerking off to a 10-second RealMedia clip    0         8  True    1077.0                                                                           Oh my god! Jerking off to a 10-second RealMedia clip killed Kenny!\n",
       "2989548         298955                      7613          False                                                                Oh my god! __ killed Kenny!                    1                Getting naked and watching Nickelodeon    0         9  True     889.0                                                                              Oh my god! Getting naked and watching Nickelodeon killed Kenny!\n",
       "2989549         298955                      7613          False                                                                Oh my god! __ killed Kenny!                    1                 Awesome pictures of planets and stuff    0        10  True     388.0                                                                               Oh my god! Awesome pictures of planets and stuff killed Kenny!\n",
       "\n",
       "[2446360 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"has_\"] = df[\"black_card_text\"].str.contains(\"_{2,}\",regex=True)\n",
    "\n",
    "df['black_card_text'] = df['black_card_text'].str.replace(\"_{2,}\",\"__\",regex=True)\n",
    "## https://stackoverflow.com/questions/47696401/replace-character-of-column-value-with-string-from-another-column-in-pandas\n",
    "## https://datascience.stackexchange.com/questions/39345/how-to-replace-a-part-string-value-of-a-column-using-another-column\n",
    "# Remove characters from one column based on string of another column\n",
    "\n",
    "## remove \".\" from end of text\n",
    "df[\"white_card_text\"] = df[\"white_card_text\"].str.replace(\"\\.$\", '',regex=True)\n",
    "df['text'] = df.apply(lambda x:x['black_card_text'].replace(\"__\", x['white_card_text']), axis=1)\n",
    "\n",
    "## add answer at end of text, in cases where no ___ in black card\n",
    "df.loc[(df[\"has_\"]==False),\"text\"] = df[\"text\"] +\" \" + df[\"white_card_text\"] #+ \".\"\n",
    "\n",
    "# removed: \n",
    "# df['text'] = df['text'].str.replace(\"..\",\".\",regex=False) ## fix double dots. Still levaes extra punct.. \n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abf3d6",
   "metadata": {},
   "source": [
    "* Add these to df_text as well\n",
    "\n",
    "Save df_text to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c31e0649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279558                                                      Next from J.K. Rowling: Harry Potter and the Chamber of Nicolas Cage.\n",
       "1628804                                                          WHOOO! God damn I love Crying and shitting and eating spaghetti!\n",
       "1011134                                                                       LSD + Getting shot by the police = really bad time.\n",
       "187356                                                       Feeling so grateful! #amazing #mylife #J.D. Power and his associates\n",
       "1519224                                                         As king, how will I keep the peasants in line? An Oedipus complex\n",
       "                                                                    ...                                                          \n",
       "897116                                                 What really killed the dinosaurs? A juicy lil' booty going poot-poot-pooty\n",
       "601515                                   Do the Dew® with our most extreme flavor yet! Get ready for Mountain Dew Mental illness!\n",
       "1553974    I swear to God I'm gonna murder my husband if he doesn't shut the fuck up about Awesome pictures of planets and stuff.\n",
       "1036166                                                                 And today's soup is Cream of Wearing Nicolas Cage's face.\n",
       "1528126                                  You see, son, baseball is like Vomiting seafood and bleeding anally. Don't overthink it.\n",
       "Length: 787683, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.concat([df_text,df['text']]).drop_duplicates()\n",
    "df_text = df_text.sample(frac=1)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41926698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_text.to_csv(\"df_text.csv.gz\",index=False,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d5b6f",
   "metadata": {},
   "source": [
    "##### Fit prior odds / encoder\n",
    "* Use targetEncoder or WOE\n",
    "* Do IPW either here, or later on data subset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e3b036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"won\"].isna().sum()==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d013408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"won\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b91c5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## for now, fit on just white cards, instead of black, white  (For black, we could just use mean_skipped as prior and multiply)\n",
    "# ## seemed to work directly on text, origianlly? \n",
    "# df[\"prior_white\"] = cat_encoder.fit_transform(X= df[\"white_card_text\"].values# df[\"id_white\"].values# pd.factorize(df.white_card_text)[0],#df[\"white_card_text\"],\n",
    "#                                               ,y=df[\"won\"].astype(int).values,groups=df[\"fake_round_id\"].values)\n",
    "\n",
    "# # df[\"prior_white\"] = cat_encoder.transform(df[\"white_card_text\"]) ## catboost encoder - doesn't have min 0 (for those with 0 cases)\n",
    "# ## rates for the whites (per unique white card)\n",
    "# df.drop_duplicates([\"white_card_text\"])[\"prior_white\"].describe(percentiles=[]).round(3)\n",
    "\n",
    "# ## why nan values ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373271e",
   "metadata": {},
   "source": [
    "### check white prior as baseline model\n",
    "\n",
    "* 21% win rate by picking most popular answer, on FILTERED Data\n",
    "* On _full_ 3 million games (including different outcomes for same pair, i.e no filtering by # picks), _it does not better than random! _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11fab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_index\n",
       "1     0.1035\n",
       "2     0.1077\n",
       "3     0.1042\n",
       "4     0.1027\n",
       "5     0.0995\n",
       "6     0.0987\n",
       "7     0.0962\n",
       "8     0.0957\n",
       "9     0.0949\n",
       "10    0.0967\n",
       "Name: won, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ID_index\"])[\"won\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e457242",
   "metadata": {},
   "source": [
    "### get min count/cooccurences of sentence pairs\n",
    "* Filter train +_ TEST for min 2 occurrences\n",
    "* Filter games/rounds where positive was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d13824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pair_count\"] = df.groupby(\"text\")[\"won\"].transform(\"count\") ## can be used to filter sentences occurring less than k times\n",
    "df[\"sum_won\"] = df.groupby(\"text\")[\"won\"].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d3e9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2446360\n",
      "2446360\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "df = df.loc[df[\"pair_count\"]>=MIN_PAIR_FREQ]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869c5d3",
   "metadata": {},
   "source": [
    "### remove groups where positive was filtered out\n",
    "( If `MIN_PAIR_FREQ` is 1, then this does nothing, all cards/games/jokes will be kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5475e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MIN_PAIR_FREQ>1:\n",
    "    print(\"Rounds before:\",df[\"fake_round_id\"].nunique())\n",
    "    any_pos = df.groupby(\"fake_round_id\")[\"won\"].transform(\"max\")>0\n",
    "    df = df.loc[any_pos]\n",
    "    print(\"Rounds After (filtering those with no pick):\",df[\"fake_round_id\"].nunique())\n",
    "    \n",
    "    ### filter round with too few cards left (we can have without all 10, but ensure no \"1\" only cases.... )\n",
    "     ## clear drop between 8,9 (17K),10 (6k) games\n",
    "    round_cards_count_mask = df.groupby(\"fake_round_id\")[\"won\"].transform(\"count\")>=8\n",
    "    df = df[round_cards_count_mask==True]\n",
    "    print(df.shape[0],\"rows after round level filter\")\n",
    "    print(df[\"fake_round_id\"].nunique(), \"rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8fbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_round_id               244636\n",
      "round_completion_seconds      1665\n",
      "round_skipped                    1\n",
      "black_card_text                581\n",
      "black_card_pick_num              1\n",
      "white_card_text               2128\n",
      "won                              2\n",
      "ID_index                        10\n",
      "has_                             2\n",
      "id_white                      2128\n",
      "text                        784974\n",
      "pair_count                      15\n",
      "sum_won                          9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d60823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    756722.00\n",
       "mean          1.31\n",
       "std           0.61\n",
       "min           1.00\n",
       "25%           1.00\n",
       "50%           1.00\n",
       "75%           1.00\n",
       "max           8.00\n",
       "Name: sum_won, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"sum_won\"]>0][\"sum_won\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17e3a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake_round_id               244636\n",
       "round_completion_seconds      1665\n",
       "round_skipped                    1\n",
       "black_card_text                581\n",
       "black_card_pick_num              1\n",
       "white_card_text               2128\n",
       "won                              2\n",
       "ID_index                        10\n",
       "has_                             2\n",
       "id_white                      2128\n",
       "text                        784974\n",
       "pair_count                      15\n",
       "sum_won                          9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaa7025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible jokes: 1236368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63.49032003416458"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Possible jokes:\",(df.black_card_text.nunique()*df.white_card_text.nunique()))\n",
    "100*df.text.nunique() / (df.black_card_text.nunique()*df.white_card_text.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471800ce",
   "metadata": {},
   "source": [
    "## Split train/test by groups\n",
    "* Later-  filter train data by label (but not the test)\n",
    "* split by round_id / games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d936234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean won: 0.1\n",
      "2446360\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(['fake_round_id', 'black_card_text', 'white_card_text',\n",
    "                'won', 'text', \"sum_won\"],axis=1)\n",
    "df = df.sort_values([\"fake_round_id\"],ascending=False) # , 'won'\n",
    "df[\"won\"] = df[\"won\"].astype(int)\n",
    "print(\"mean won:\",df[\"won\"].mean())\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982b71e",
   "metadata": {},
   "source": [
    "## Alt: split by unique cards\n",
    "* unique / novel white, black card combinations\n",
    "* Randomly select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2206c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_whites 2128 test whites: 425\n"
     ]
    }
   ],
   "source": [
    "all_blacks = df[\"black_card_text\"].unique()\n",
    "# print(len(all_blacks))\n",
    "# test_blacks = np.random.choice(all_blacks,size=int(len(all_blacks)*0.05), replace=False)\n",
    "\n",
    "all_whites = df[\"white_card_text\"].unique()\n",
    "\n",
    "test_whites = np.random.choice(all_whites,size=int(len(all_whites)*0.2), replace=False)\n",
    "print(\"all_whites\",len(all_whites),\"test whites:\",len(test_whites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51066de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows 1956418\n",
      "test rows 489942\n",
      "df_train_cards mean won 0.09971028686098779\n",
      "df_test_cards mean won 0.1011568716297031\n"
     ]
    }
   ],
   "source": [
    "card_mask = (df[\"white_card_text\"].isin(test_whites))\n",
    "df_test_cards = df.loc[card_mask].copy()\n",
    "df_train_cards = df.loc[~card_mask].copy()\n",
    "\n",
    "print(\"train rows\",df_train_cards.shape[0])\n",
    "print(\"test rows\",df_test_cards.shape[0])\n",
    "print(\"df_train_cards mean won\",df_train_cards.won.mean())\n",
    "print(\"df_test_cards mean won\",df_test_cards.won.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7656a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TRAIN_DATA:\n",
    "    df_train_cards.to_parquet(\"cah_train_cardsplit_games.parquet\")\n",
    "    df_test_cards.to_parquet(\"cah_test_cardsplit_games.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cfa2f",
   "metadata": {},
   "source": [
    "### \"Normal\" round-level train/test split:\n",
    "\n",
    "* ALT - below - take last k rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "459701cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244636"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fake_round_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cd4c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).sort_values(\"fake_round_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ec3e4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2446360"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] # 2,446,360 rows  , and 298,955 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d607c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>text</th>\n",
       "      <th>sum_won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387656</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Crippling social anxiety</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Crippling social anxiety</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387654</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Slowly releasing a huge fart over the course of two minutes</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Slowly releasing a huge fart over the course of two minutes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387653</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Crab</td>\n",
       "      <td>1</td>\n",
       "      <td>What's about to take this dance floor to the next level? Crab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387650</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Trimming the poop out of Chewbacca's butt hair</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Trimming the poop out of Chewbacca's butt hair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387652</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Subduing a grizzly bear and making her your wife</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Subduing a grizzly bear and making her your wife</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387657</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>The NRA</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? The NRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387658</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Fucking a corpse back to life</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Fucking a corpse back to life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387659</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Denying climate change</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Denying climate change</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387651</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Fisting</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Fisting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387655</th>\n",
       "      <td>238766</td>\n",
       "      <td>What's about to take this dance floor to the next level?</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>0</td>\n",
       "      <td>What's about to take this dance floor to the next level? Elon Musk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387663</th>\n",
       "      <td>238767</td>\n",
       "      <td>Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about __!</td>\n",
       "      <td>A fun, sexy time at the nude beach</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about A fun, sexy time at the nude beach!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387660</th>\n",
       "      <td>238767</td>\n",
       "      <td>Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about __!</td>\n",
       "      <td>Giving birth to the Antichrist</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about Giving birth to the Antichrist!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fake_round_id                                                                                                             black_card_text                                              white_card_text  won                                                                                                                                                        text  sum_won\n",
       "2387656         238766                                                                    What's about to take this dance floor to the next level?                                     Crippling social anxiety    0                                                                           What's about to take this dance floor to the next level? Crippling social anxiety        0\n",
       "2387654         238766                                                                    What's about to take this dance floor to the next level?  Slowly releasing a huge fart over the course of two minutes    0                                        What's about to take this dance floor to the next level? Slowly releasing a huge fart over the course of two minutes        1\n",
       "2387653         238766                                                                    What's about to take this dance floor to the next level?                                                         Crab    1                                                                                               What's about to take this dance floor to the next level? Crab        1\n",
       "2387650         238766                                                                    What's about to take this dance floor to the next level?               Trimming the poop out of Chewbacca's butt hair    0                                                     What's about to take this dance floor to the next level? Trimming the poop out of Chewbacca's butt hair        0\n",
       "2387652         238766                                                                    What's about to take this dance floor to the next level?             Subduing a grizzly bear and making her your wife    0                                                   What's about to take this dance floor to the next level? Subduing a grizzly bear and making her your wife        0\n",
       "2387657         238766                                                                    What's about to take this dance floor to the next level?                                                      The NRA    0                                                                                            What's about to take this dance floor to the next level? The NRA        1\n",
       "2387658         238766                                                                    What's about to take this dance floor to the next level?                                Fucking a corpse back to life    0                                                                      What's about to take this dance floor to the next level? Fucking a corpse back to life        0\n",
       "2387659         238766                                                                    What's about to take this dance floor to the next level?                                       Denying climate change    0                                                                             What's about to take this dance floor to the next level? Denying climate change        0\n",
       "2387651         238766                                                                    What's about to take this dance floor to the next level?                                                      Fisting    0                                                                                            What's about to take this dance floor to the next level? Fisting        0\n",
       "2387655         238766                                                                    What's about to take this dance floor to the next level?                                                    Elon Musk    0                                                                                          What's about to take this dance floor to the next level? Elon Musk        0\n",
       "2387663         238767  Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about __!                           A fun, sexy time at the nude beach    0  Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about A fun, sexy time at the nude beach!        0\n",
       "2387660         238767  Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about __!                               Giving birth to the Antichrist    1      Hey there, Young Scientists! Put on your labcoats and strap on your safety goggles, because today we're learning about Giving birth to the Antichrist!        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size = df.shape[0]//5 ## 20% \n",
    "## round to nearest 10\n",
    "test_size = int(math.ceil(test_size / 10.0)) * 10\n",
    "\n",
    "df_test = df.tail(test_size)\n",
    "print(df_test.shape[0])\n",
    "display(df_test.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfd1f5",
   "metadata": {},
   "source": [
    "#### subset df to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fce4e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1957080"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.iloc[:-test_size]\n",
    "df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f918098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## # Initialize the GroupShuffleSplit.\n",
    "# gss = GroupShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "\n",
    "# # Get the indexers for the split.\n",
    "# idx1, idx2 = next(gss.split(df, groups=df[\"fake_round_id\"].values))\n",
    "\n",
    "# # Get the split DataFrames.\n",
    "# df, df_test = df.iloc[idx1], df.iloc[idx2]\n",
    "\n",
    "# print(\"df (train)\",df.shape[0])\n",
    "# print(df[\"fake_round_id\"].nunique())\n",
    "# print(\"test\")\n",
    "# print(\"df_test\",df_test.shape[0])\n",
    "# print(df_test[\"fake_round_id\"].nunique())\n",
    "\n",
    "# df_train = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206d381",
   "metadata": {},
   "source": [
    "### save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4576c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TRAIN_DATA:\n",
    "#     df.to_parquet(\"cah_train_min3_games.parquet\")\n",
    "#     df_test.to_parquet(\"cah_test_min3_games.parquet\")\n",
    "#     df.to_parquet(\"cah_train_games.parquet\")\n",
    "    \n",
    "    df_train.to_parquet(\"cah_train_games.parquet\")\n",
    "    df_test.to_parquet(\"cah_test_games.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea520232",
   "metadata": {},
   "source": [
    "#### Whites prior: \n",
    "* 20.5% accuracy on test set!!;\n",
    "* 23% after min 3 filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bf2c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489280 test shape after join\n",
      "Prior Acc @1: 0.2072841726618705\n",
      "alt dedup Prior Acc @1: 0.2072841726618705\n",
      "Prior Acc @2: 0.36032537606278614\n",
      "Prior Acc @3: 0.48606115107913667\n"
     ]
    }
   ],
   "source": [
    "# df_white_prior = df.groupby([\"white_card_text\"], as_index=False)[\"won\"].mean().rename(columns={\"won\":\"white_prior\"}).set_index(\"white_card_text\")\n",
    "df_white_prior = df_train.groupby([\"white_card_text\"], as_index=False)[\"won\"].mean().rename(columns={\"won\":\"white_prior\"}).set_index(\"white_card_text\")\n",
    "df_test = df_test.join(df_white_prior,on=\"white_card_text\",how=\"left\")\n",
    "prior = df_test[\"white_prior\"].mean()\n",
    "# df_test[\"white_prior\"] = df_test[\"white_prior\"].fillna(prior)\n",
    "df_test[\"white_prior\"] = df_test[\"white_prior\"].fillna(0.1)\n",
    "print(df_test.shape[0],\"test shape after join\")\n",
    "print(\"Prior Acc @1:\",df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean())\n",
    "## alkt: \n",
    "print(\"alt dedup Prior Acc @1:\",df_test.sort_values(\"white_prior\",ascending=False).drop_duplicates(subset=[\"fake_round_id\"])[\"won\"].mean())\n",
    "# print(\"Prior Acc @1:\",df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(1).groupby(\"fake_round_id\")[\"won\"].max().mean()) ## res identical to above\n",
    "print(\"Prior Acc @2:\",df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(2).groupby(\"fake_round_id\")[\"won\"].max().mean())\n",
    "print(\"Prior Acc @3:\",df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(3).groupby(\"fake_round_id\")[\"won\"].max().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1ce14",
   "metadata": {},
   "source": [
    "### keep 1s (for train) ?\n",
    "* MAY want to filter - e.g. min # cases of it being picked (+- keeping that for use for sample weight)\n",
    "* Keep all data for now, (to allow for grouping by rounds if we want, since this is a ranking problem) \n",
    "\n",
    "* temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd4d8d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_round_id      195708\n",
      "black_card_text       581\n",
      "white_card_text      2128\n",
      "won                     2\n",
      "white_counts          213\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>white_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2989504</th>\n",
       "      <td>298951</td>\n",
       "      <td>Every Tuesday, I purchase a box of donuts. I sit on the toilet. I eat the donuts. I remember __, and I cry.</td>\n",
       "      <td>Slowly easing down onto a cucumber</td>\n",
       "      <td>1</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798560</th>\n",
       "      <td>179857</td>\n",
       "      <td>Ain't it nifty? Barb and Bob hit 50! So get off your ass and raise a glass to 50 years of __.</td>\n",
       "      <td>Ejaculating a quart of hollandaise sauce</td>\n",
       "      <td>1</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798439</th>\n",
       "      <td>179844</td>\n",
       "      <td>When asked about the biggest threat facing the nation, 60% of Americans said __.</td>\n",
       "      <td>Being sad and horny</td>\n",
       "      <td>1</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798441</th>\n",
       "      <td>179845</td>\n",
       "      <td>Kids, I don't need drugs to get high. I'm high on __.</td>\n",
       "      <td>Dumpster juice</td>\n",
       "      <td>1</td>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798459</th>\n",
       "      <td>179846</td>\n",
       "      <td>Click Here for __!!!</td>\n",
       "      <td>Everything</td>\n",
       "      <td>1</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794108</th>\n",
       "      <td>179411</td>\n",
       "      <td>And what did you bring for show and tell?</td>\n",
       "      <td>A fuck-ton of almonds</td>\n",
       "      <td>0</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794107</th>\n",
       "      <td>179411</td>\n",
       "      <td>And what did you bring for show and tell?</td>\n",
       "      <td>Nazis</td>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794106</th>\n",
       "      <td>179411</td>\n",
       "      <td>And what did you bring for show and tell?</td>\n",
       "      <td>German dungeon porn</td>\n",
       "      <td>0</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794105</th>\n",
       "      <td>179411</td>\n",
       "      <td>And what did you bring for show and tell?</td>\n",
       "      <td>A toxic family environment</td>\n",
       "      <td>0</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.</td>\n",
       "      <td>Mufasa's death scene</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1957076 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fake_round_id                                                                                              black_card_text                           white_card_text  won  white_counts\n",
       "2989504         298951  Every Tuesday, I purchase a box of donuts. I sit on the toilet. I eat the donuts. I remember __, and I cry.        Slowly easing down onto a cucumber    1           981\n",
       "1798560         179857                Ain't it nifty? Barb and Bob hit 50! So get off your ass and raise a glass to 50 years of __.  Ejaculating a quart of hollandaise sauce    1          1012\n",
       "1798439         179844                             When asked about the biggest threat facing the nation, 60% of Americans said __.                       Being sad and horny    1          1014\n",
       "1798441         179845                                                        Kids, I don't need drugs to get high. I'm high on __.                            Dumpster juice    1          1021\n",
       "1798459         179846                                                                                         Click Here for __!!!                                Everything    1          1038\n",
       "...                ...                                                                                                          ...                                       ...  ...           ...\n",
       "1794108         179411                                                                    And what did you bring for show and tell?                     A fuck-ton of almonds    0          1062\n",
       "1794107         179411                                                                    And what did you bring for show and tell?                                     Nazis    0          1006\n",
       "1794106         179411                                                                    And what did you bring for show and tell?                       German dungeon porn    0          1011\n",
       "1794105         179411                                                                    And what did you bring for show and tell?                A toxic family environment    0          1025\n",
       "9                    1                    Hi MTV! My name is Kendra, I live in Malibu, I'm into __, and I love to have a good time.                      Mufasa's death scene    0           977\n",
       "\n",
       "[1957076 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr = df[['fake_round_id',\"black_card_text\",\"white_card_text\",\"won\"]].sort_values(\"won\",ascending=False).copy()\n",
    "df_tr =df_tr.drop_duplicates(subset=['fake_round_id',\"black_card_text\",\"white_card_text\"])\n",
    "df_tr[\"white_counts\"] = df_tr.groupby(\"white_card_text\")[\"won\"].transform(\"size\")\n",
    "print(df_tr.nunique())\n",
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cef02f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1957076.0\n",
       "mean         997.9\n",
       "std           67.0\n",
       "min           10.0\n",
       "25%          978.0\n",
       "50%         1001.0\n",
       "75%         1023.0\n",
       "max         1969.0\n",
       "Name: white_counts, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr[\"white_counts\"].describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-amino",
   "metadata": {},
   "source": [
    "## Sentence transformers model(s)\n",
    "* Sentence bert (NLI? semtantic similarity?) \n",
    "https://www.sbert.net/docs/training/overview.html\n",
    "https://www.sbert.net/docs/quickstart.html  , https://github.com/UKPLab/sentence-transformers\n",
    "* Load data from disk ? \n",
    "\n",
    "* list of pretrained sentencebert models: (All are >300 M! slow download) https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0\n",
    "\n",
    "* `stsb-distilroberta-base-v2` - 305M sized model - semantic similarity\n",
    "* `nli-distilroberta-base-v2` - NLI \n",
    "* `average_word_embeddings_glove.6B.300d`, `average_word_embeddings_glove.840B.300d` - glove/w2v embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdd949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, evaluation\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "## https://stackoverflow.com/questions/50307707/convert-pandas-dataframe-to-pytorch-tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "## example from : https://www.sbert.net/docs/training/overview.html\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "consecutive-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pretrained weights - download vis program often fails, easier to download externally from ftp\n",
    "# model = SentenceTransformer('stsb-distilroberta-base-v2') ## was distilbert-base-nli-mean-tokens\n",
    "\n",
    "# model = SentenceTransformer(\"nli-distilroberta-base-v2\",device=\"cuda\")\n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\") ## \"paraphrase-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\"\n",
    "# model = SentenceTransformer(\"./stsb-distilroberta-base-v2\",device=\"cuda\")\n",
    "\n",
    "# model = SentenceTransformer(\"./nli-mpnet-base-v2\",device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6026b18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "534e4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model.tokenize([\"ad and \"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "legal-exemption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_sentence_embedding_dimension() # 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-repeat",
   "metadata": {},
   "source": [
    "#### redo above, but with real data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "modified-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244635\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "TEST_SIZE = df.shape[0]//8 #4500\n",
    "print(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9bb652d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1957080, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "711f4206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746559"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(['black_card_text', 'white_card_text']).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "## is list correct? error otherwise? \n",
    "train_examples = list(df.iloc[TEST_SIZE:].apply(lambda row: InputExample(texts=[row[\"white_card_text\"], row[\"black_card_text\"]],\n",
    "                                                                         label=row[\"picks\"]),axis=1)) # picks\n",
    "test_examples =  list(df.iloc[0:TEST_SIZE].apply(lambda row: InputExample(texts=[row[\"white_card_text\"], row[\"black_card_text\"]], \n",
    "                                                                          label=row[\"picks\"]),axis=1)) # picks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_eval = evaluation.BinaryClassificationEvaluator(sentences1=df.iloc[0:2500,0],\n",
    "#     sentences2= df.iloc[0:2500,1],\n",
    "#     labels=df.iloc[0:2500,2],show_progress_bar=True,batch_size=256)\n",
    "dev_eval = evaluation.BinaryClassificationEvaluator(sentences1=list(df.iloc[0:TEST_SIZE,0].values),\n",
    "    sentences2= list(df.iloc[0:TEST_SIZE,1].values),\n",
    "    labels=list(df.iloc[0:TEST_SIZE,2].values),show_progress_bar=True,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli.py \n",
    "\n",
    "## evaluator example \n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-bones",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_loss = losses.CosineSimilarityLoss(model) # ORIG\n",
    "\n",
    "# train_loss = losses.SoftmaxLoss(model,\n",
    "#     sentence_embedding_dimension= model.get_sentence_embedding_dimension(),\n",
    "#     num_labels= 2,\n",
    "#     concatenation_sent_rep= True,\n",
    "#     concatenation_sent_difference = True,\n",
    "#     concatenation_sent_multiplication = True) # ALT - for classification https://www.sbert.net/docs/package_reference/losses.html#softmaxloss\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "          epochs=num_epochs, \n",
    "#           warmup_steps=100, # 100\n",
    "          warmup_steps = math.ceil(len(train_dataloader) * 0.05*3), #5% of train data for warm-up\n",
    "          evaluator=dev_eval,\n",
    "          use_amp=True,\n",
    "          optimizer_params= {'lr': 3e-04}, # 2e-05\n",
    "          output_path=\"./output\",\n",
    "          save_best_model = False,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(dev_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8eb98",
   "metadata": {},
   "source": [
    "#### Evaluate model on original data, game by game.\n",
    "* Note: should use proper test set , disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc659d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('cah_lab_data.parquet')\n",
    "df_test = df_sample.loc[df_sample[\"black_card_pick_num\"]==1].sort_values(\"fake_round_id\")[[\"fake_round_id\",\"black_card_text\",\"white_card_text\",\"won\"]].copy()\n",
    "\n",
    "df_test = df_test.head(4800)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00316b2b",
   "metadata": {},
   "source": [
    "* if using just black/white, could do this for unique cards - ~1000 x less compute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "\n",
    "sentences1 = df_test[\"black_card_text\"].values\n",
    "sentences2 = df_test[\"white_card_text\"].values\n",
    "embeddings1 = model.encode(sentences1,batch_size=256,normalize_embeddings=False, convert_to_tensor=True)\n",
    "\n",
    "embeddings2 = model.encode(sentences2,batch_size=256,normalize_embeddings=False, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99badd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "    scores.append(float(cosine_scores[i][i]))\n",
    "df_test[\"cos_scores\"] = scores\n",
    "df_test.sort_values([\"fake_round_id\",\"cos_scores\"],inplace=True,ascending=True)\n",
    "print(\"top1 accuracy:\",df_test.groupby(\"fake_round_id\").head(1)[\"won\"].mean())\n",
    "\n",
    "df_test.sort_values([\"fake_round_id\",\"cos_scores\"],inplace=True,ascending=False)\n",
    "print(\"top1 accuracy, reverse sort:\",df_test.groupby(\"fake_round_id\").head(1)[\"won\"].mean())\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
