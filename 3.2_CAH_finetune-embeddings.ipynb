{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liIk8f980xT5"
   },
   "source": [
    "# Sentence Embeddings using Siamese BERT-Networks/Sentence Transformers\n",
    "* Generic model, get embeddings, finetune, evaluate over card split\n",
    "\n",
    "This version also combined white/black pairs and trains over a single column/text\n",
    "* https://datascience.stackexchange.com/questions/39345/how-to-replace-a-part-string-value-of-a-column-using-another-column\n",
    "* NOTE: Some cards lack a \"____\" - need to handle them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIjj9BjkY_A9",
    "outputId": "c0e5c2cb-618d-45fc-8aa1-675544d669e1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzljhyTQEZds"
   },
   "source": [
    "## Install Sentence Transformer Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmxRYxNDvn6y",
    "outputId": "a94618d9-5eec-416d-888a-d8616462fc43"
   },
   "outputs": [],
   "source": [
    "# # Install the library using pip\n",
    "# !pip3 install sentence-transformers scikit-learn -U\n",
    "# !pip3 install nltk -U\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jS9O0eAwlt8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score  \n",
    "\n",
    "# https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FcJ95uWKXoJC"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses, SentencesDataset ## https://www.sbert.net/docs/package_reference/losses.html + SentencesDataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses ## MultipleNegativesRankingLoss\n",
    "## https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss\n",
    "# from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "import os\n",
    "import gzip\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1gRQUdycC9aR"
   },
   "outputs": [],
   "source": [
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "model_name = \"output/cah_tsdae-model\"##\"/content/drive/MyDrive/research/cah/cah_tsdae-model\"\n",
    "min_cooccurences = 3 # filter sentences for pairs that occurred at least k times. min 5: 200K. min 1: 1.9M\n",
    "\n",
    "ONE_COL_DATA_FORMAT = True\n",
    "USE_TEXT_COLS =  [\"text\",\"white_card_text\"]#[\"black_card_text\",\t\"white_card_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIAKz6KVEndZ"
   },
   "source": [
    "## Load the sBERT Model\n",
    "\n",
    "* Default , later try pretrained+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5IO_j2Ofv5pq"
   },
   "outputs": [],
   "source": [
    "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
    "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
    "\n",
    "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "# model = SentenceTransformer(\"nli-distilroberta-base-v2\")\n",
    "## # https://www.sbert.net/docs/pretrained_models.html\n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") ## \"paraphrase-MiniLM-L12-v2\"\n",
    "# model = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "model = SentenceTransformer(model_name)\n",
    "# model = SentenceTransformer(\"all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7_Ib3ITEwgO"
   },
   "source": [
    "## Setup a Corpus\n",
    "\n",
    "* `/content/drive/MyDrive/Research/CAH/cah_train_min7_v1.csv` - 37K\n",
    "    * `/content/drive/MyDrive/Research/CAH/cah_min7_v2.csv.gz`\n",
    "* `/content/drive/MyDrive/Research/CAH/cah_train_min6_v1.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7tGxkF4l-oB",
    "outputId": "453aab5f-78ea-4eb1-f806-3171e9646d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008030 rows\n",
      "              won  pair_count\n",
      "count  1008030.00  1008030.00\n",
      "mean         0.11        3.51\n",
      "std          0.32        1.39\n",
      "min          0.00        1.00\n",
      "25%          0.00        3.00\n",
      "50%          0.00        3.00\n",
      "75%          0.00        4.00\n",
      "max          1.00       13.00\n",
      "1008030\n"
     ]
    }
   ],
   "source": [
    "## \"/content/drive/MyDrive/Research/CAH/cah_min6_v2.csv.gz\"\n",
    "# \"/content/drive/MyDrive/Research/CAH/cah_min2_v2.csv.gz\"\n",
    "# \"/content/drive/MyDrive/Research/CAH/cah_train_min4_v1.csv\"\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/Research/CAH/cah_min6_v2.csv.gz\",usecols=[\"black_card_text\",\"white_card_text\",\"picks\"])#.sample(99)\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/Research/CAH/cah_train_min4_v1.csv\",usecols=[\"black_card_text\",\"white_card_text\",\"picks\"])#.sample(99)\n",
    "\n",
    "TRAIN_PATH = \"cah_train_cardsplit_games.parquet\" # \"/content/drive/MyDrive/research/cah/cah_train_games.parquet\"\n",
    "df = pd.read_parquet(TRAIN_PATH).drop(columns=[\"prior_white\"],errors=\"ignore\") \n",
    "\n",
    "df[\"won\"] = df[\"won\"].astype(int)\n",
    "df = df.sort_values(by=\"won\",ascending=False) ## get picked pairs first\n",
    "\n",
    "# df_all = df.copy() ## copy for quick eval\n",
    "print(df.shape[0],\"rows\")\n",
    "\n",
    "df[\"pair_count\"] = df.groupby(\"text\")[\"won\"].transform(\"count\") ## can be used to filter sentences occurring less than k times\n",
    "print(df[[\"won\",\"pair_count\"]].describe().round(2) )\n",
    "\n",
    "# df = df.drop_duplicates(subset=[\"black_card_text\",\"white_card_text\"],keep=\"first\")#.sample(frac=1)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyWPiyqQBphJ"
   },
   "source": [
    "Test set\n",
    "* Keeps round level grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "IxyngBX8ACCc",
    "outputId": "2b8b3049-0762-4e5a-c071-63ce736de7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_round_id      129753\n",
      "black_card_text       396\n",
      "white_card_text      1960\n",
      "won                     2\n",
      "text               107390\n",
      "sum_won                 8\n",
      "dtype: int64\n",
      "312663 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake_round_id</th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>text</th>\n",
       "      <th>sum_won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2473311</th>\n",
       "      <td>247332</td>\n",
       "      <td>Get ready for the movie of the summer! One cop...</td>\n",
       "      <td>Not being a part of my son's life</td>\n",
       "      <td>0</td>\n",
       "      <td>Get ready for the movie of the summer! One cop...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653395</th>\n",
       "      <td>65340</td>\n",
       "      <td>It's beginning to look a lot like __.</td>\n",
       "      <td>Going to prom with a 108-year-old vampire</td>\n",
       "      <td>0</td>\n",
       "      <td>It's beginning to look a lot like Going to pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933200</th>\n",
       "      <td>193321</td>\n",
       "      <td>Hey baby, come back to my place and I'll show ...</td>\n",
       "      <td>A positive male role model</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey baby, come back to my place and I'll show ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907963</th>\n",
       "      <td>90797</td>\n",
       "      <td>I'm just gonna stay in tonight. You know, Netf...</td>\n",
       "      <td>Filling a man's anus with concrete</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm just gonna stay in tonight. You know, Netf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306475</th>\n",
       "      <td>30648</td>\n",
       "      <td>Well what do you have to say for yourself, Cas...</td>\n",
       "      <td>A horse with no legs</td>\n",
       "      <td>0</td>\n",
       "      <td>Well what do you have to say for yourself, Cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231342</th>\n",
       "      <td>223135</td>\n",
       "      <td>I have solved politics. My solution is __.</td>\n",
       "      <td>Falling into a pit of waffles</td>\n",
       "      <td>0</td>\n",
       "      <td>I have solved politics. My solution is Falling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076273</th>\n",
       "      <td>207628</td>\n",
       "      <td>This is the way the world ends This is the way...</td>\n",
       "      <td>Chunky highlights</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the way the world ends This is the way...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406780</th>\n",
       "      <td>40679</td>\n",
       "      <td>A study published in Nature this week found th...</td>\n",
       "      <td>Mooing</td>\n",
       "      <td>0</td>\n",
       "      <td>A study published in Nature this week found th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169372</th>\n",
       "      <td>116938</td>\n",
       "      <td>Up next on Nickelodeon: \"Clarissa Explains __.\"</td>\n",
       "      <td>Poopy diapers</td>\n",
       "      <td>0</td>\n",
       "      <td>Up next on Nickelodeon: \"Clarissa Explains Poo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359317</th>\n",
       "      <td>35932</td>\n",
       "      <td>What sounds great after four margaritas?</td>\n",
       "      <td>Meatloaf, the man</td>\n",
       "      <td>1</td>\n",
       "      <td>What sounds great after four margaritas? Meatl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312663 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fake_round_id                                    black_card_text  \\\n",
       "2473311         247332  Get ready for the movie of the summer! One cop...   \n",
       "653395           65340              It's beginning to look a lot like __.   \n",
       "1933200         193321  Hey baby, come back to my place and I'll show ...   \n",
       "907963           90797  I'm just gonna stay in tonight. You know, Netf...   \n",
       "306475           30648  Well what do you have to say for yourself, Cas...   \n",
       "...                ...                                                ...   \n",
       "2231342         223135         I have solved politics. My solution is __.   \n",
       "2076273         207628  This is the way the world ends This is the way...   \n",
       "406780           40679  A study published in Nature this week found th...   \n",
       "1169372         116938    Up next on Nickelodeon: \"Clarissa Explains __.\"   \n",
       "359317           35932           What sounds great after four margaritas?   \n",
       "\n",
       "                                   white_card_text  won  \\\n",
       "2473311          Not being a part of my son's life    0   \n",
       "653395   Going to prom with a 108-year-old vampire    0   \n",
       "1933200                 A positive male role model    0   \n",
       "907963          Filling a man's anus with concrete    0   \n",
       "306475                        A horse with no legs    0   \n",
       "...                                            ...  ...   \n",
       "2231342              Falling into a pit of waffles    0   \n",
       "2076273                          Chunky highlights    0   \n",
       "406780                                      Mooing    0   \n",
       "1169372                              Poopy diapers    0   \n",
       "359317                           Meatloaf, the man    1   \n",
       "\n",
       "                                                      text  sum_won  \n",
       "2473311  Get ready for the movie of the summer! One cop...        2  \n",
       "653395   It's beginning to look a lot like Going to pro...        0  \n",
       "1933200  Hey baby, come back to my place and I'll show ...        0  \n",
       "907963   I'm just gonna stay in tonight. You know, Netf...        0  \n",
       "306475   Well what do you have to say for yourself, Cas...        0  \n",
       "...                                                    ...      ...  \n",
       "2231342  I have solved politics. My solution is Falling...        1  \n",
       "2076273  This is the way the world ends This is the way...        1  \n",
       "406780   A study published in Nature this week found th...        0  \n",
       "1169372  Up next on Nickelodeon: \"Clarissa Explains Poo...        1  \n",
       "359317   What sounds great after four margaritas? Meatl...        1  \n",
       "\n",
       "[312663 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"/content/drive/MyDrive/research/cah/cah_test_games.parquet\"\n",
    "df_test = pd.read_parquet(\"cah_test_cardsplit_games.parquet\").drop(columns=[\"prior_white\"],errors=\"ignore\").sample(frac=1)\n",
    "\n",
    "df_test[\"won\"] = df_test[\"won\"].astype(int)\n",
    "print(df_test.nunique())\n",
    "print(df_test.shape[0],\"rows\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUh_gOK32mZu"
   },
   "source": [
    "### check mean baseline prior\n",
    "* By min cooccurrences of sentence pairs in in raw data\n",
    "1 min occ Prior Acc: 0.2044\n",
    "2 min occ Prior Acc: 0.2032\n",
    "3 min occ Prior Acc: 0.2027\n",
    "4 min occ Prior Acc: 0.2011\n",
    "5 min occ Prior Acc: 0.1922\n",
    "6 min occ Prior Acc: 0.1762\n",
    "7 min occ Prior Acc: 0.1503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRC7aCEE2mBe",
    "outputId": "fe840841-29b6-4c1b-bca7-e3cebc9acbfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 min occ, 1008030 rows\n",
      "White Prior Acc: 0.122\n",
      "Pair Prior (Only)  Acc: 0.116\n",
      "White then Pair Prior Acc: 0.121\n",
      "Pair Prior then White Acc: 0.121\n",
      "2 min occ, 966844 rows\n",
      "White Prior Acc: 0.122\n",
      "Pair Prior (Only)  Acc: 0.116\n",
      "White then Pair Prior Acc: 0.121\n",
      "Pair Prior then White Acc: 0.121\n",
      "3 min occ, 771250 rows\n",
      "White Prior Acc: 0.121\n",
      "Pair Prior (Only)  Acc: 0.116\n",
      "White then Pair Prior Acc: 0.121\n",
      "Pair Prior then White Acc: 0.121\n",
      "4 min occ, 445801 rows\n",
      "White Prior Acc: 0.122\n",
      "Pair Prior (Only)  Acc: 0.116\n",
      "White then Pair Prior Acc: 0.121\n",
      "Pair Prior then White Acc: 0.121\n",
      "5 min occ, 212557 rows\n",
      "White Prior Acc: 0.121\n",
      "Pair Prior (Only)  Acc: 0.116\n",
      "White then Pair Prior Acc: 0.121\n",
      "Pair Prior then White Acc: 0.121\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,9):\n",
    "#   df_temp = df.loc[df[\"pair_count\"] >=i]\n",
    "#   df_white_prior = df_temp.groupby([\"white_card_text\"], as_index=False)[\"won\"].mean().rename(columns={\"won\":\"white_prior\"}).set_index(\"white_card_text\")\n",
    "#   df_test = df_test.join(df_white_prior,on=\"white_card_text\",how=\"left\")\n",
    "#   prior = df_test[\"white_prior\"].mean()\n",
    "#   df_test[\"white_prior\"] = df_test[\"white_prior\"].fillna(prior)\n",
    "#   print(f\"{i} min occ Prior Acc: %.3f , {df_temp.shape[0]} rows\" %df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.6% (with min 6) , 17.6% with min 3\n",
    "#   df_test.drop(columns=[\"white_prior\"],errors=\"ignore\",inplace=True)\n",
    "\n",
    "for i in range(1,6):\n",
    "    df_temp = df.loc[df[\"pair_count\"] >=i].copy()\n",
    "    print(f\"{i} min occ, {df_temp.shape[0]} rows\")\n",
    "    df_white_prior = df_temp.groupby([\"white_card_text\"], as_index=False)[\"won\"].mean().rename(columns={\"won\":\"white_prior\"}).set_index(\"white_card_text\")\n",
    "    df_test = df_test.join(df_white_prior,on=\"white_card_text\",how=\"left\")\n",
    "    prior = df_test[\"white_prior\"].mean()\n",
    "    df_test[\"white_prior\"] = df_test[\"white_prior\"].fillna(prior)\n",
    "    print(\"White Prior Acc: %.3f\" %df_test.sort_values(\"white_prior\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.6% (with min 6) , 17.6% with min 3\n",
    "\n",
    "    ## prior for a black-white combination - mean (freq% won), or sum (times won?) , or threshholded max? (over 2 times?)\n",
    "    df_pair_prior = df_temp.groupby([\"white_card_text\",\"black_card_text\"], as_index=False)[\"won\"].sum().rename(columns={\"won\":\"pair_prior\"}).set_index([\"white_card_text\",\"black_card_text\"])\n",
    "\n",
    "    df_test = df_test.join(df_pair_prior,on=[\"white_card_text\",\"black_card_text\"],how=\"left\")\n",
    "    prior = df_test[\"pair_prior\"].mean()\n",
    "    df_test[\"pair_prior\"] = df_test[\"pair_prior\"].fillna(prior)\n",
    "    print(\"Pair Prior (Only)  Acc: %.3f\" %df_test.sort_values(\"pair_prior\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.6% (with min 6) , 17.6% with min 3\n",
    "    print(\"White then Pair Prior Acc: %.3f\" %df_test.sort_values([\"white_prior\",\"pair_prior\"],ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.6% (with min 6) , 17.6% with min 3\n",
    "    print(\"Pair Prior then White Acc: %.3f\" %df_test.sort_values([\"pair_prior\",\"white_prior\",],ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.6% (with min 6) , 17.6% with min 3\n",
    "\n",
    "    df_test.drop(columns=[\"white_prior\",\"pair_prior\"],errors=\"ignore\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhvJzhL2BeFM"
   },
   "source": [
    "Drop duplicate instances with same out put (i.e ignore round level/ranking) \n",
    "* keep positives preferrably\n",
    "* Could do : weight or filter bby # occurrences\n",
    "\n",
    "* 22% mean win rate after this (instea of 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "OKAwvWRzBctu",
    "outputId": "666d3d4b-fd4c-426b-b4d7-d3ac884f6f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_round_id      142659\n",
      "black_card_text       377\n",
      "white_card_text      1571\n",
      "won                     2\n",
      "text               205520\n",
      "sum_won                 9\n",
      "pair_count             11\n",
      "dtype: int64\n",
      "771250 rows after 3 filter of pairs\n",
      "mean won: 0.331758466329311\n",
      "black_card_text       377\n",
      "white_card_text      1571\n",
      "won                     2\n",
      "text               205520\n",
      "sum_won                 9\n",
      "dtype: int64\n",
      "205520 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black_card_text</th>\n",
       "      <th>white_card_text</th>\n",
       "      <th>won</th>\n",
       "      <th>text</th>\n",
       "      <th>sum_won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2535065</th>\n",
       "      <td>What never fails to liven up the party?</td>\n",
       "      <td>Whispering all sexy</td>\n",
       "      <td>0</td>\n",
       "      <td>What never fails to liven up the party? Whispe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712581</th>\n",
       "      <td>What's making things awkward in the sauna?</td>\n",
       "      <td>Gay conversion therapy</td>\n",
       "      <td>0</td>\n",
       "      <td>What's making things awkward in the sauna? Gay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593710</th>\n",
       "      <td>You love Black Friday. You love Cyber Monday. ...</td>\n",
       "      <td>Blackface</td>\n",
       "      <td>1</td>\n",
       "      <td>You love Black Friday. You love Cyber Monday. ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896899</th>\n",
       "      <td>I've had a horrible vision, father. I saw moun...</td>\n",
       "      <td>Sucking the caviar straight out of a fish's pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>I've had a horrible vision, father. I saw moun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301071</th>\n",
       "      <td>What's fun until it gets weird?</td>\n",
       "      <td>Barely legal boys</td>\n",
       "      <td>1</td>\n",
       "      <td>What's fun until it gets weird? Barely legal boys</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045990</th>\n",
       "      <td>Computer! Display __ on screen. Enhance.</td>\n",
       "      <td>Smashing my balls at the moment of climax</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer! Display Smashing my balls at the mom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738538</th>\n",
       "      <td>Sure, sex is great, but have you tried __?</td>\n",
       "      <td>Homework</td>\n",
       "      <td>0</td>\n",
       "      <td>Sure, sex is great, but have you tried Homework?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886179</th>\n",
       "      <td>Say it loud! I'm __ and I'm proud!</td>\n",
       "      <td>The human body</td>\n",
       "      <td>0</td>\n",
       "      <td>Say it loud! I'm The human body and I'm proud!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600947</th>\n",
       "      <td>And what did you bring for show and tell?</td>\n",
       "      <td>A hit new fantasy show called \"Penis Man.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>And what did you bring for show and tell? A hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618536</th>\n",
       "      <td>Goldfinger! He's the man, the man with __.</td>\n",
       "      <td>Taking a man's eyes and balls out and putting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Goldfinger! He's the man, the man with Taking ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205520 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           black_card_text  \\\n",
       "2535065            What never fails to liven up the party?   \n",
       "2712581         What's making things awkward in the sauna?   \n",
       "1593710  You love Black Friday. You love Cyber Monday. ...   \n",
       "2896899  I've had a horrible vision, father. I saw moun...   \n",
       "301071                     What's fun until it gets weird?   \n",
       "...                                                    ...   \n",
       "1045990           Computer! Display __ on screen. Enhance.   \n",
       "2738538         Sure, sex is great, but have you tried __?   \n",
       "2886179                 Say it loud! I'm __ and I'm proud!   \n",
       "2600947          And what did you bring for show and tell?   \n",
       "2618536         Goldfinger! He's the man, the man with __.   \n",
       "\n",
       "                                           white_card_text  won  \\\n",
       "2535065                                Whispering all sexy    0   \n",
       "2712581                             Gay conversion therapy    0   \n",
       "1593710                                          Blackface    1   \n",
       "2896899  Sucking the caviar straight out of a fish's pussy    0   \n",
       "301071                                   Barely legal boys    1   \n",
       "...                                                    ...  ...   \n",
       "1045990          Smashing my balls at the moment of climax    1   \n",
       "2738538                                           Homework    0   \n",
       "2886179                                     The human body    0   \n",
       "2600947         A hit new fantasy show called \"Penis Man.\"    0   \n",
       "2618536  Taking a man's eyes and balls out and putting ...    0   \n",
       "\n",
       "                                                      text  sum_won  \n",
       "2535065  What never fails to liven up the party? Whispe...        0  \n",
       "2712581  What's making things awkward in the sauna? Gay...        0  \n",
       "1593710  You love Black Friday. You love Cyber Monday. ...        3  \n",
       "2896899  I've had a horrible vision, father. I saw moun...        0  \n",
       "301071   What's fun until it gets weird? Barely legal boys        1  \n",
       "...                                                    ...      ...  \n",
       "1045990  Computer! Display Smashing my balls at the mom...        1  \n",
       "2738538   Sure, sex is great, but have you tried Homework?        0  \n",
       "2886179     Say it loud! I'm The human body and I'm proud!        0  \n",
       "2600947  And what did you bring for show and tell? A hi...        0  \n",
       "2618536  Goldfinger! He's the man, the man with Taking ...        0  \n",
       "\n",
       "[205520 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## filter for sentence pairs occurring at least X times, regardless of label\n",
    "df = df.loc[df[\"pair_count\"] >=min_cooccurences]\n",
    "print(df.nunique())\n",
    "print(df.shape[0],f\"rows after {min_cooccurences} filter of pairs\")\n",
    "df = df.drop(columns=[\"fake_round_id\",\"prior_white\",\"pair_count\"],errors=\"ignore\") # drop round id if not doing group level ranking\n",
    "df = df.sort_values(by=\"won\",ascending=False).drop_duplicates(subset=[\"black_card_text\",\"white_card_text\"],keep=\"first\").sample(frac=1)\n",
    "print(\"mean won:\",df[\"won\"].mean())\n",
    "print(df.nunique())\n",
    "print(df.shape[0],\"rows\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "szXA3q0VWDjg"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# df[[#'black_card_text',\n",
    "#     # 'white_card_text',\n",
    "#   'text','won']].rename(columns={'won':\"target\"}).to_csv(f\"cah_min{min_cooccurences}.csv.gz\",compression=\"gzip\",index=False,quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUVdkcJ7hFu3"
   },
   "source": [
    "#### train - eval split (if doing supervised pretraining... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLDoriAMFDRE"
   },
   "source": [
    "##### Data formats: \n",
    "1. 2 text cols\n",
    "  * Can be white, black\n",
    "  * could be merged (`text`) and white/black,\n",
    "2. single joint text col\n",
    "  * Merged (`text`) col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Msv2dCOphFIv"
   },
   "outputs": [],
   "source": [
    "# y = df[\"won\"].values\n",
    "# X = np.concatenate([s1_emb,s2_emb],axis=1) # BOTH \n",
    "# X = list(df[\"text\"].values)\n",
    "\n",
    "if ONE_COL_DATA_FORMAT:\n",
    "  ### 1 text col version\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df[\"text\"].values), list(df[\"won\"].astype(int).values), test_size=0.1, random_state=42)\n",
    "\n",
    "#     train_samples=  [InputExample(texts=[X_train[i]],label=float(y_train[i])) for i in range(len(X_train))]\n",
    "\n",
    "# else:\n",
    "#   ### 2 text col version ; can try different cols\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     list(df[USE_TEXT_COLS].values), list(df[\"won\"].values), test_size=0.1, random_state=42)\n",
    "  \n",
    "# #   train_samples=  [InputExample(texts=[X_train[i][0],X_train[i][1]],label=float(y_train[i])) for i in range(len(X_train))]\n",
    "#   # test_samples=  [InputExample(texts=[X_test[i]][0],X_test[i]][1]],label=float(y_test[i])) for i in range(len(X_test))]\n",
    "#   ### binary evaluator: expect list 1, list 2:\n",
    "#   ## https://www.sbert.net/docs/package_reference/evaluation.html\n",
    "# #   dev_eval = evaluation.BinaryClassificationEvaluator(sentences1=[x[0] for x in X_test],sentences2 = [x[1] for x in X_test],labels = y_test,\n",
    "# #                                                                           batch_size=128,show_progress_bar=True,write_csv=True)\n",
    "# # train_examples =  [InputExample(texts=[X_train[i]],label=float(y_train[i])) for i in range(len(X_train))]\n",
    "# # train_examples = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck8HAVmMZRli"
   },
   "source": [
    "* See how well unsupervised model does? \n",
    "* Check if funny combs are close or far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GO_PRyRAhVN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184968 tr\n",
      "20552 test\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),\"tr\")\n",
    "print(len(X_test),\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c10ac61b7a24485a65d4d092bc6cb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b75c2d83e7848bcb90d0e04cceb0b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#X_test_emb\n",
    "X_test = model.encode(X_test,\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=64\n",
    "                    #   convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "\n",
    "X_train = model.encode(X_train,\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=64\n",
    "                    #   convert_to_tensor=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NZWgLq44wVT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.97      0.80    123485\n",
      "           1       0.52      0.06      0.11     61483\n",
      "\n",
      "    accuracy                           0.67    184968\n",
      "   macro avg       0.60      0.52      0.45    184968\n",
      "weighted avg       0.62      0.67      0.57    184968\n",
      "\n",
      "roc_auc 0.5954\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# StandardScaler\n",
    "clf = make_pipeline(StandardScaler(),  LogisticRegression(solver=\"sag\")) ## PCA(n_components=2), # error with array? \n",
    "# clf = LogisticRegression()\n",
    "# cv_preds = cross_val_predict(clf,X,y,n_jobs=-2)\n",
    "\n",
    "cv_preds = cross_val_predict(clf,X_train,y_train,method=\"predict_proba\",cv=3,n_jobs=-2)[:,1]\n",
    "print(classification_report(y_true=y_train,y_pred=cv_preds>0.5))\n",
    "print(\"roc_auc %.4f\" %roc_auc_score(y_true=y_train,y_score=cv_preds))\n",
    "\n",
    "\n",
    "# #### min 4 (joint text), v1, 371K records\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.65      0.90      0.75    227115\n",
    "#            1       0.58      0.22      0.32    144299\n",
    "\n",
    "#     accuracy                           0.64    371414\n",
    "#    macro avg       0.61      0.56      0.54    371414\n",
    "# weighted avg       0.62      0.64      0.58    371414\n",
    "\n",
    "# roc_auc 0.620\n",
    "\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.78      0.70     50158\n",
    "#            1       0.57      0.38      0.46     37673\n",
    "\n",
    "#     accuracy                           0.61     87831\n",
    "#    macro avg       0.60      0.58      0.58     87831\n",
    "# weighted avg       0.60      0.61      0.59     87831\n",
    "\n",
    "# roc_auc 0.62310\n",
    "# Wall time: 6min 21s\n",
    "\n",
    "### min 6, with delta instead of white card features: same results as above\n",
    "\n",
    "# ### min 6 + with some delta, diff, and B+W + cosine sim[0]\n",
    "\n",
    "## min 6 (Above) + nli-distilroberta-base-v2:\n",
    "## roc_auc 0.634\n",
    "\n",
    "# ### min 7\n",
    "# # ## with some delta, diff, and B+W + cosine sim[0]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.68      0.64     20290\n",
    "#            1       0.57      0.48      0.52     17670\n",
    "\n",
    "#     accuracy                           0.59     37960\n",
    "#    macro avg       0.59      0.58      0.58     37960\n",
    "# weighted avg       0.59      0.59      0.59     37960\n",
    "\n",
    "# roc_auc 0.6182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C571NK8Ozcqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80    123485\n",
      "           1       0.00      0.00      0.00     61483\n",
      "\n",
      "    accuracy                           0.67    184968\n",
      "   macro avg       0.33      0.50      0.40    184968\n",
      "weighted avg       0.45      0.67      0.53    184968\n",
      "\n",
      "roc_auc 0.5000\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## slow... ? \n",
    "clf_rf = RandomForestClassifier(n_jobs=-2,n_estimators=200,\n",
    "    max_depth=14,    min_samples_split=3,    min_samples_leaf=2,ccp_alpha=0.05,class_weight=\"balanced\")\n",
    "\n",
    "cv_preds_2 = cross_val_predict(clf_rf,X_train,y_train,method=\"predict_proba\",cv=3)[:,1]\n",
    "print(classification_report(y_true=y_train,y_pred=cv_preds_2>0.5))\n",
    "print(\"roc_auc %.4f\" %roc_auc_score(y_true=y_train,y_score=cv_preds_2))\n",
    "\n",
    "# # #### min 4 (joint text), v1, 371K records\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.94      0.76    227115\n",
    "#            1       0.61      0.14      0.23    144299\n",
    "\n",
    "#     accuracy                           0.63    371414\n",
    "#    macro avg       0.62      0.54      0.49    371414\n",
    "# weighted avg       0.62      0.63      0.55    371414\n",
    "\n",
    "# roc_auc 0.609\n",
    "######\n",
    "# ## 10 ,min on min6/80k\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.64      0.78      0.70     50158\n",
    "#            1       0.59      0.42      0.49     37673\n",
    "\n",
    "#     accuracy                           0.63     87831\n",
    "#    macro avg       0.62      0.60      0.60     87831\n",
    "# weighted avg       0.62      0.63      0.61     87831\n",
    "\n",
    "# roc_auc 0.65095\n",
    "# Wall time: 9min 52s\n",
    "\n",
    "### min 6, with delta instead of white card features: \n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.64      0.76      0.70     50158\n",
    "#            1       0.58      0.43      0.50     37673\n",
    "\n",
    "#     accuracy                           0.62     87831\n",
    "#    macro avg       0.61      0.60      0.60     87831\n",
    "# weighted avg       0.62      0.62      0.61     87831\n",
    "\n",
    "# roc_auc 0.64848\n",
    "# Wall time: 14min 5s\n",
    "\n",
    "# # ### min 6 + with some delta, diff, and B+W + cosine sim[0]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.64      0.77      0.70     50001\n",
    "#            1       0.59      0.43      0.49     37830\n",
    "\n",
    "#     accuracy                           0.62     87831\n",
    "#    macro avg       0.61      0.60      0.60     87831\n",
    "# weighted avg       0.62      0.62      0.61     87831\n",
    "\n",
    "# roc_auc 0.6518\n",
    "\n",
    "## min 6 (Above) + nli-distilroberta-base-v2:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.64      0.78      0.70     50001\n",
    "#            1       0.59      0.43      0.49     37830\n",
    "\n",
    "#     accuracy                           0.62     87831\n",
    "#    macro avg       0.62      0.60      0.60     87831\n",
    "# weighted avg       0.62      0.62      0.61     87831\n",
    "\n",
    "# roc_auc 0.6520\n",
    "\n",
    "# ### min 7\n",
    "# ## with some delta, diff, and B+W + cosine sim[0]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.71      0.66     20290\n",
    "#            1       0.59      0.48      0.53     17670\n",
    "\n",
    "#     accuracy                           0.60     37960\n",
    "#    macro avg       0.60      0.60      0.59     37960\n",
    "# weighted avg       0.60      0.60      0.60     37960\n",
    "# roc_auc 0.634\n",
    "\n",
    "\n",
    "\n",
    "##TODO - feature importance (do the deltas improve model?)\n",
    "\n",
    "# min 2 , L12 miniLM\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.73      0.97      0.83    469455\n",
    "#            1       0.53      0.10      0.17    183561\n",
    "\n",
    "#     accuracy                           0.72    653016\n",
    "#    macro avg       0.63      0.53      0.50    653016\n",
    "# weighted avg       0.68      0.72      0.65    653016\n",
    "\n",
    "# roc_auc 0.64871\n",
    "# CPU times: user 2h "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on true held out set\n",
    "* Disjoint by cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean eval won: 0.116\n"
     ]
    }
   ],
   "source": [
    "y_eval = list(df_test[\"won\"].astype(int).values)\n",
    "print(\"mean eval won: %.3f\" %np.mean(y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a314b678ea40e4aa21d26adb8f850f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 48 min\n",
    "X_eval = model.encode(list(df_test[\"text\"].values), \n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=64\n",
    "                    #   convert_to_tensor=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80    123485\n",
      "           1       0.00      0.00      0.00     61483\n",
      "\n",
      "    accuracy                           0.67    184968\n",
      "   macro avg       0.33      0.50      0.40    184968\n",
      "weighted avg       0.45      0.67      0.53    184968\n",
      "\n",
      "roc_auc 0.5000\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clf = clf_rf\n",
    "clf = make_pipeline(StandardScaler(),  LogisticRegression(solver=\"sag\",class_weight=\"balanced\"))\n",
    "clf.fit(X_train,y_train)\n",
    "eval_preds = clf.predict_proba(X_eval)[:,1]\n",
    "print(classification_report(y_true=y_train,y_pred=cv_preds_2>0.5))\n",
    "print(\"roc_auc %.4f\" % roc_auc_score(y_true=y_train,y_score=cv_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80    123485\n",
      "           1       0.00      0.00      0.00     61483\n",
      "\n",
      "    accuracy                           0.67    184968\n",
      "   macro avg       0.33      0.50      0.40    184968\n",
      "weighted avg       0.45      0.67      0.53    184968\n",
      "\n",
      "roc_auc 0.5000\n",
      "Wall time: 5min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clf = clf_rf\n",
    "clf = LogisticRegressionCV(solver=\"sag\",class_weight=\"balanced\",n_jobs=-2)\n",
    "clf.fit(X_train,y_train)\n",
    "eval_preds = clf.predict_proba(X_eval)[:,1]\n",
    "print(classification_report(y_true=y_train,y_pred=cv_preds_2>0.5))\n",
    "print(\"roc_auc %.4f\" % roc_auc_score(y_true=y_train,y_score=cv_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1345864835495133\n"
     ]
    }
   ],
   "source": [
    "df_test[\"preds\"] = eval_preds\n",
    "print(df_test.sort_values([\"preds\"],ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 0.134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1345864835495133\n"
     ]
    }
   ],
   "source": [
    "# clf = clf_rf\n",
    "# clf.fit(X_train,y_train)\n",
    "# eval_preds = clf.predict_proba(X_eval)[:,1]\n",
    "# df_test[\"preds\"] = eval_preds\n",
    "# print(df_test.sort_values([\"preds\"],ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 0.115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11578923030681372\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569,
     "referenced_widgets": [
      "f055766d35ab4f7c9dfa40e8abc54376",
      "e8a1f0b90f2d4ed1bf34b25fb70de489",
      "e62d97f441d747f589829ba0f5b1261c",
      "f0d4f7a5fc0244f28d1b07bfee8b9746",
      "f9c01e2dce6a46b389ebe3b366658a98",
      "7b941caa3b674402af30c1b62d3eba71",
      "3a077b1da9574591983ba37e9839e876",
      "0673d974ee8a44ec8978ba73aeade63b",
      "d1a58901df8640798cdf4d97ba376b45",
      "d16e87bc97664db8a9b1fa7a3fa8bdfe",
      "39e01e2aa6704920864de0d34459bf9c"
     ]
    },
    "id": "fZpw55M5ZaN2",
    "outputId": "6d494175-f3c5-440b-ed2a-abc279736962"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### could speed this up n^2 by getting embeddings only for unique combniations, then rejoining/merging...  (Won't work if using \"Text\" col)\n",
    "# s1_emb = model.encode(list(df[USE_TEXT_COLS[0]].values),\n",
    "#                       normalize_embeddings=True,\n",
    "#                       show_progress_bar=True,\n",
    "#                       batch_size=128\n",
    "#                     #   convert_to_tensor=True\n",
    "#                     )\n",
    "\n",
    "# ## black cards = quesiton/prompt\n",
    "# s2_emb = model.encode(list(df[USE_TEXT_COLS[1]].values),\n",
    "#                       normalize_embeddings=True,\n",
    "#                       show_progress_bar=True,\n",
    "#                       batch_size=256\n",
    "#                     #   convert_to_tensor=True\n",
    "#                     )#.astype(np.float32) ## 1.5 min for 4K , with L12 miniLM, on cpu\n",
    "\n",
    "# dot_sim = []\n",
    "# for i in range (len(s1_emb)):\n",
    "#     dot_sim.append(float(util.dot_score(s1_emb[i], s2_emb[i])[0]))\n",
    "# print(len(dot_sim))\n",
    "\n",
    "# df[\"dot_sim_score\"] = dot_sim\n",
    "# # df_all[\"dot_sim_score\"] = dot_sim\n",
    "# print(df[[\"dot_sim_score\",\"won\"]].corr())\n",
    "# print(\"df:\\n\",df.groupby(\"won\")[\"dot_sim_score\"].mean())\n",
    "\n",
    "# # print(\"df_All:\")\n",
    "# # print(df_all[[\"dot_sim_score\",\"won\"]].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uMXZ6ctoh-2-"
   },
   "outputs": [],
   "source": [
    "# dot_sim = []\n",
    "# for i in range (len(s1_emb)):\n",
    "#     dot_sim.append(float(util.dot_score(s1_emb[i], s2_emb[i])[0]))\n",
    "# print(len(dot_sim))\n",
    "\n",
    "# df[\"dot_sim_score\"] = dot_sim\n",
    "# # df_all[\"dot_sim_score\"] = dot_sim\n",
    "# print(df[[\"dot_sim_score\",\"won\"]].corr())\n",
    "# print(\"df:\\n\",df.groupby(\"won\")[\"dot_sim_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3dNDnt06ae9Q"
   },
   "outputs": [],
   "source": [
    "# print(\"df_all:\\n\",df_all.groupby(\"won\")[\"dot_sim_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHa6wGqNghi5"
   },
   "source": [
    "### try supervised model\n",
    "\n",
    "* Contrastive loss?\n",
    "* https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/quora_duplicate_questions/training_OnlineContrastiveLoss.py\n",
    "* https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#training\n",
    "\n",
    "\n",
    "Can combine multiple losses:\n",
    "* https://www.sbert.net/examples/training/quora_duplicate_questions/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "o7Gzd4DmkB0N"
   },
   "outputs": [],
   "source": [
    "#As distance metric, we use cosine distance (cosine_distance = 1-cosine_similarity)\n",
    "distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE\n",
    "\n",
    "#Negative pairs should have a distance of at least 0.3 (was 0.5 orig)\n",
    "margin = 0.5\n",
    "\n",
    "# ####  Configure the training #### \n",
    "# warmup_steps = math.ceil(len(train_dataset) * num_epochs / batch_size * 0.1) # 10% of train data for warm-up\n",
    "# print(\"Warmup-steps: {}\".format(warmup_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJWvGNY7BJhY"
   },
   "outputs": [],
   "source": [
    "## results before finetuning\n",
    "dev_eval(model) ## % with 7e-4 and min 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuDei5zLgWFR"
   },
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(train_samples, model=model)\n",
    "# DataLoader to batch your data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "## OnlineContrastiveLoss. - expects pairs\n",
    "# train_loss = losses.OnlineContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "# Call the fit method\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    # weight_decay=0,\n",
    "    scheduler='constantlr', \n",
    "    optimizer_params={'lr': 5e-4 ## 3e-5\n",
    "                      },\n",
    "    show_progress_bar=True,\n",
    "    use_amp=True,\n",
    "    evaluator=dev_eval,\n",
    "    output_path='./cah_sbert_cos',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufLRWUIWkZ-I"
   },
   "outputs": [],
   "source": [
    "### dev eval\n",
    "## example: https://jovian.ai/vumichien/sbert\n",
    "dev_eval(model) ## 0.555 Accracy (With min 6) , 57% with 1e-3 and min 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38dBeQHac722"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CDwk0GxdAk0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### could speed this up n^2 by getting embeddings only for unique combniations, then rejoining/merging...  (Won't work if using \"Text\" col)\n",
    "s1_emb = model.encode(list(df[USE_TEXT_COLS[0]].values),\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=256,\n",
    "                    #   convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "## black cards = quesiton/prompt\n",
    "s2_emb = model.encode(list(df[USE_TEXT_COLS[1]].values),\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=256,\n",
    "                    #   convert_to_tensor=True\n",
    "                    )#.astype(np.float32) ## 1.5 min for 4K , with L12 miniLM, on cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04SnpNCFeDb4"
   },
   "outputs": [],
   "source": [
    "\n",
    "dot_sim = []\n",
    "cos_score = []\n",
    "for i in range (len(s1_emb)):\n",
    "    # dot_sim.append(util.dot_score(s1_emb[i], s2_emb[i])[0])\n",
    "    dot_sim.append(float(util.dot_score(s1_emb[i], s2_emb[i])[0])) ## am I getting the right cell with these two?? (cos and dot, [i][j] ? )\n",
    "    cos_score.append(float(util.cos_sim(s1_emb[i], s2_emb[i])[0]))\n",
    "print(len(dot_sim))\n",
    "\n",
    "df[\"dot_sim_score\"] = dot_sim\n",
    "df[\"cos_sim_score\"] = cos_score\n",
    "print(df[[\"dot_sim_score\",\"cos_sim_score\",\"won\"]].corr().round(3))\n",
    "\n",
    "print(\"df:\\n\",df.groupby(\"won\")[[\"dot_sim_score\",\"cos_sim_score\"]].mean().round(3))\n",
    "\n",
    "# print(\"df_All:\")\n",
    "# df_all[\"dot_sim_score\"] = dot_sim\n",
    "# print(df_all[[\"dot_sim_score\",\"won\"]].corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eT_LQFIiOhb"
   },
   "source": [
    "## Eval on df_Test\n",
    "* Can speed up by getting unique sentence pairs and their embeddings\n",
    "* Won't work the same for single text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv4dAPxkiXQK"
   },
   "outputs": [],
   "source": [
    "# df_test.drop_duplicates([\"black_card_text\",\"white_card_text\"]).shape[0] # 367K rows, vs 489 K for all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6H_8-Ldirmw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### could speed this up n^2 by getting embeddings only for unique combniations, then rejoining/merging...  (Won't work if using \"Text\" col)\n",
    "s1_emb = model.encode(list(df_test[USE_TEXT_COLS[0]].values),\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=256,\n",
    "                    #   convert_to_tensor=True\n",
    "                    )\n",
    "\n",
    "## black cards = quesiton/prompt\n",
    "s2_emb = model.encode(list(df_test[USE_TEXT_COLS[1]].values),\n",
    "                      normalize_embeddings=False,\n",
    "                      show_progress_bar=True,\n",
    "                      batch_size=256,\n",
    "                    #   convert_to_tensor=True\n",
    "                    )#.astype(np.float32) ## 1.5 min for 4K , with L12 miniLM, on cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWBwiv7mirq-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dot_sim = []\n",
    "cos_score = []\n",
    "for i in range (len(s1_emb)):\n",
    "    # dot_sim.append(util.dot_score(s1_emb[i], s2_emb[i])[0])\n",
    "    dot_sim.append(float(util.dot_score(s1_emb[i], s2_emb[i])[0])) ## am I getting the right cell with these two?? (cos and dot, [i][j] ? )\n",
    "    cos_score.append(float(util.cos_sim(s1_emb[i], s2_emb[i])[0]))\n",
    "print(len(dot_sim))\n",
    "\n",
    "df_test[\"dot_sim_score\"] = dot_sim\n",
    "df_test[\"cos_sim_score\"] = cos_score\n",
    "print(df_test[[\"dot_sim_score\",\"cos_sim_score\",\"won\"]].corr().round(3))\n",
    "\n",
    "print(\"df_test:\\n\",df_test.groupby(\"won\")[[\"dot_sim_score\",\"cos_sim_score\"]].mean().round(3))\n",
    "\n",
    "\n",
    "print(\"Acc:\",df_test.sort_values(\"dot_sim_score\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 17.5% acc\n",
    "print(\"Acc: (dot)\",df_test.sort_values(\"dot_sim_score\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean())\n",
    "print(\"Acc:\",df_test.sort_values(\"cos_sim_score\",ascending=True).groupby(\"fake_round_id\").head(1)[\"won\"].mean()) ## 0.05\n",
    "print(\"Acc:\",df_test.sort_values(\"cos_sim_score\",ascending=False).groupby(\"fake_round_id\").head(1)[\"won\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkIPD7_uXw6D"
   },
   "source": [
    "### Pretrain - unsupervised\n",
    "* TSDAE or other method ? \n",
    "* https://www.sbert.net/examples/unsupervised_learning/TSDAE/README.html#tsdae-as-pre-training-task\n",
    "\n",
    "\n",
    "ST recommends MultipleNegativesRankingLoss\n",
    "* https://www.sbert.net/examples/training/nli/README.html#multiplenegativesrankingloss\n",
    "*  MultipleNegativesRankingLoss only requires positive pairs, i.e., we only need examples of positive/funny pairs. (BUT It also supports hard negatives in a triplet)\n",
    "\n",
    "Also: OnlineContrastiveLoss\n",
    "* https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#training\n",
    "* Constrative Loss / `losses.OnlineContrastiveLoss`\n",
    "* Choosing the distance function and especially choosing a sensible margin are quite important for the success of constrative loss. In the given example, we use cosine_distance (which is 1-cosine_similarity) with a margin of 0.5. I.e., non-duplicate questions should have a cosine_distance of at least 0.5 (which is equivalent to a 0.5 cosine similarity difference).\n",
    "* An improved version of constrative loss is OnlineConstrativeLoss, which looks which negative pairs have a lower distance that the largest positive pair and which positive pairs have a higher distance than the lowest distance of negative pairs. I.e., this loss automatically detects the hard cases in a batch and computes the loss only for these cases.\n",
    "\n",
    "Can also do BOTH losses, as in:\n",
    "\n",
    "* Multi-Task-Learning\n",
    "    Constrative Loss works well for pair classification, i.e., given two pairs, are these duplicates or not. It pushes negative pairs far away in vector space, so that the distinguishing between duplicate and non-duplicate pairs works good.\n",
    "\n",
    "    MultipleNegativesRankingLoss on the other sides mainly reduces the distance between positive pairs out of large set of possible candidates. However, the distance between non-duplicate questions is not so large, so that this loss does not work that weill for pair classification.\n",
    "\n",
    "* https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#multi-task-learning\n",
    "\n",
    "More losses (e.g. triplet): https://www.sbert.net/docs/package_reference/losses.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrObJFWJkM4n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAlPTP7_XwVp"
   },
   "outputs": [],
   "source": [
    "# # Define your sentence transformer model using CLS pooling\n",
    "# model_name = 'bert-base-uncased'\n",
    "# word_embedding_model = models.Transformer(model_name)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
    "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# # Define a list with sentences (1k - 100k sentences)\n",
    "# train_sentences = [\"Your set of sentences\",\n",
    "#                    \"Model will automatically add the noise\", \n",
    "#                    \"And re-construct it\",\n",
    "#                    \"You should provide at least 1k sentences\"]\n",
    "\n",
    "# # Create the special denoising dataset that adds noise on-the-fly\n",
    "# train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "\n",
    "# # DataLoader to batch your data\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # Use the denoising auto-encoder loss\n",
    "# train_loss = losses.DenoisingAutoEncoderLoss(model, decoder_name_or_path=model_name, tie_encoder_decoder=True)\n",
    "\n",
    "# # Call the fit method\n",
    "# model.fit(\n",
    "#     train_objectives=[(train_dataloader, train_loss)],\n",
    "#     epochs=1,\n",
    "#     weight_decay=0,\n",
    "#     scheduler='constantlr',\n",
    "#     optimizer_params={'lr': 3e-5},\n",
    "#     show_progress_bar=True\n",
    "# )\n",
    "\n",
    "# model.save('output/tsdae-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UwMrw__mpbH"
   },
   "source": [
    "#### get embeddings & dot product/codine distance over all \n",
    "\n",
    "* `util.semantic_search` - could do this is one step for us (but we'll want to train a model on embeddings anyway) \n",
    "\n",
    "* https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-4YyJH1mwOJ"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## white cards = most predictive information\n",
    "\n",
    "# # ### encode_multi_process ? (needs pool enabled)\n",
    "# ##     pool = model.start_multi_process_pool(encode_batch_size=2000)\n",
    "# ##     embeddings = model.encode_multi_process(texts, pool)\n",
    "\n",
    "# s1_emb = model.encode(list(df[\"white_card_text\"].values),\n",
    "#                       normalize_embeddings=True,\n",
    "#                       show_progress_bar=True,\n",
    "#                       batch_size=128,\n",
    "#                     #   convert_to_tensor=True \n",
    "#                       )#.astype(np.float32) ## 40s for 4K , with L12 miniLM, on cpu\n",
    "# ### 7 min for all min7 (37k) with L12\n",
    "# print(s1_emb.shape,\"s1_emb.shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c84GZCIOm6rc"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### 8.7 min on colab for L12, min 4 (371K)\n",
    "\n",
    "# s1_emb = model.encode(list(df[\"white_card_text\"].values),\n",
    "#                       normalize_embeddings=True,\n",
    "#                       show_progress_bar=True,\n",
    "#                       batch_size=128,\n",
    "#                       normalize_embeddings=True\n",
    "#                     #   convert_to_tensor=True\n",
    "#                     )\n",
    "\n",
    "# ## black cards = quesiton/prompt\n",
    "# s2_emb = model.encode(list(df[\"black_card_text\"].values),\n",
    "#                       normalize_embeddings=True,\n",
    "#                       show_progress_bar=True,\n",
    "#                       batch_size=256,\n",
    "#                       normalize_embeddings=True\n",
    "#                     #   convert_to_tensor=True\n",
    "#                     )#.astype(np.float32) ## 1.5 min for 4K , with L12 miniLM, on cpu\n",
    "# print(s2_emb.shape)\n",
    "\n",
    "\n",
    "# dot_sim = []\n",
    "# for i in range (len(s1_emb)):\n",
    "#     dot_sim.append(util.dot_score(s1_emb[i], s2_emb[i])[0])\n",
    "# print(len(dot_sim))\n",
    "# dot_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HS3rT63uW6J"
   },
   "outputs": [],
   "source": [
    "# len(s2_emb[0]) # 384 = dim of the \"first\" sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUejcarOuocO"
   },
   "outputs": [],
   "source": [
    "# ### Hopefully I am doing this correctly ??? \n",
    "# ### get cosine similarity \n",
    "# output = []\n",
    "# for i in range (len(s1_emb)):\n",
    "#     output.append(util.cos_sim(s1_emb[i], s2_emb[i])[0])\n",
    "# print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQYpl70Cn9aP"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Compute cosine similarity between all pairs - outputs matrix of shape S1 X S2 (i.e # samples = ineffecient in memory!)\n",
    "# output = util.pytorch_cos_sim(s1_emb, s2_emb)\n",
    "# output = util.cos_sim(s1_emb, s2_emb) # ORIG, nXn matrix\n",
    "# print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8plFSPvsN2C"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### Hopefully I am doing this correctly ??? \n",
    "# # # ## dot product:\n",
    "# # dot_sim = util.dot_score(s1_emb, s2_emb) # memory crash? \n",
    "# # print(dot_sim.shape)\n",
    "\n",
    "# dot_sim = []\n",
    "# for i in range (len(s1_emb)):\n",
    "#     dot_sim.append(util.dot_score(s1_emb[i], s2_emb[i])[0])\n",
    "# print(len(dot_sim))\n",
    "# dot_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dO5Fro2AEJY"
   },
   "outputs": [],
   "source": [
    "# print(\"cos done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ewqEcHfqgGW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# from torch.nn import CosineSimilarity, PairwiseDistance\n",
    "# ## https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "# nn_cos = CosineSimilarity(dim=1, eps=1e-6)\n",
    "# output = nn_cos(s1_emb, s2_emb) \n",
    "# print(\"output\",output.shape)\n",
    "# nn_pairwiseDist = PairwiseDistance()\n",
    "# output_2 = nn_pairwiseDist(s1_emb, s2_emb)\n",
    "# print(\"output_2\",output_2.shape)\n",
    "\n",
    "# # ## dot product:\n",
    "# output_3 = util.dot_score(s1_emb, s2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bpVLWPqqgM4"
   },
   "outputs": [],
   "source": [
    "# df[\"cos_sim\"] = output\n",
    "# df[\"pairwiseDist_sim\"] = output_2\n",
    "# df[\"dot_score\"] = output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pV02TX0q9H8"
   },
   "outputs": [],
   "source": [
    "# df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mJQsyAPrIPh"
   },
   "outputs": [],
   "source": [
    "# df.groupby(\"picks\")[[\"cos_sim\",\"pairwiseDist_sim\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj7QLQmVt-7S"
   },
   "source": [
    "#### Model on embeddings \n",
    "* linear model on embeddings per sentence and cossim, pairwise dist score;\n",
    "* + difference, +- multiplication of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kZfxLXWEF93"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### mean diff, math/ mult per row: \n",
    "\n",
    "# ## https://stackoverflow.com/questions/50430585/mean-difference-of-two-numpy-arrays\n",
    "# # np.mean(np.abs(s1_emb[:, None] - s2_emb))\n",
    "# vector_diffs = s1_emb - s2_emb\n",
    "# mean_diff = np.mean(vector_diffs,axis=1) # 1 col\n",
    "# max_diff = np.min(vector_diffs,axis=1) \n",
    "# min_diff = np.max(vector_diffs,axis=1) \n",
    "# # np_dot = np.dot(s1_emb,s2_emb)\n",
    "# print(mean_diff)\n",
    "# print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HurL_DrAK68"
   },
   "outputs": [],
   "source": [
    "y = df[\"picks\"].values\n",
    "# X = np.concatenate([s1_emb,s2_emb],axis=1) # BOTH \n",
    "X = s2_emb # BOTH \n",
    "# X = np.concatenate([s1_emb,vector_diffs],axis=1)## ALT - white cards + diffs\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWxHrgvdt9kj"
   },
   "outputs": [],
   "source": [
    "# X = np.column_stack((X,output)) ### memory crash ? \n",
    "# X = np.column_stack((X,dot_sim)) ## dot product \n",
    "\n",
    "# # X = np.column_stack((X,output_2))\n",
    "# # X = np.column_stack((X,output_3))\n",
    "# X = np.column_stack((X,mean_diff))\n",
    "# X = np.column_stack((X,max_diff))\n",
    "# X = np.column_stack((X,min_diff))\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIEDabG88ax8"
   },
   "outputs": [],
   "source": [
    "df[\"text\"].str.split().str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVhmVMGq3Vvi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# y = df[\"picks\"].values\n",
    "# # X = np.concatenate([s1_emb,s2_emb],axis=1) # BOTH \n",
    "# X = list(df[\"black_card_text\"].values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df[\"text\"].values), list(df[\"picks\"].values), test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WEZ5crL80SI"
   },
   "outputs": [],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPs1eIc14Ei0"
   },
   "outputs": [],
   "source": [
    "# Try finetuning? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdKgjj3tzcx4"
   },
   "outputs": [],
   "source": [
    "### https://www.sbert.net/docs/training/overview.html\n",
    "from torch import nn\n",
    "\n",
    "# word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=128, activation_function=nn.Tanh())\n",
    "\n",
    "# model2 = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "model2 = SentenceTransformer(modules=[model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5gL5gVG8rAt"
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lZuN3eO5Ex7"
   },
   "outputs": [],
   "source": [
    "# [X_train[0:5],y_train[0:5]]\n",
    "## list(df[\"black_card_text\"].values)\n",
    "train_examples =  [InputExample(texts=[X_train[i]],label=float(y_train[i])) for i in range(len(X_train)-1)]\n",
    "# [X_train[0:5],y_train[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqwId_8b7zel"
   },
   "outputs": [],
   "source": [
    "range(len(X_train)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC_4K4HK7vDg"
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAQC1-uI7sA3"
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o8dxHLV7osN"
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKBO4Rj67lsZ"
   },
   "outputs": [],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smasDpEz38-E"
   },
   "outputs": [],
   "source": [
    "### IndexError: list index out of range. ? \n",
    "\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "# train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "#     InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "# train_examples =  [X_train[0:5],y_train[0:5]]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
    "# train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# #Tune the model\n",
    "# model2.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izHedCfQ985E"
   },
   "outputs": [],
   "source": [
    "### https://www.sbert.net/docs/package_reference/losses.html#batchsemihardtripletloss\n",
    "## supports 1 sentence/label pair\n",
    "train_dataset = SentencesDataset(train_examples, model2)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=256)\n",
    "train_loss = losses.BatchSemiHardTripletLoss(model=model2)\n",
    "\n",
    "\n",
    "#Tune the model\n",
    "model2.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLZaLMDuGhEh"
   },
   "source": [
    "## Perform Semantic Search"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Uj7QLQmVt-7S"
   ],
   "name": "3-CAH_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0673d974ee8a44ec8978ba73aeade63b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39e01e2aa6704920864de0d34459bf9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a077b1da9574591983ba37e9839e876": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b941caa3b674402af30c1b62d3eba71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d16e87bc97664db8a9b1fa7a3fa8bdfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1a58901df8640798cdf4d97ba376b45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e62d97f441d747f589829ba0f5b1261c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a077b1da9574591983ba37e9839e876",
      "placeholder": "​",
      "style": "IPY_MODEL_7b941caa3b674402af30c1b62d3eba71",
      "value": "Batches:   2%"
     }
    },
    "e8a1f0b90f2d4ed1bf34b25fb70de489": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f055766d35ab4f7c9dfa40e8abc54376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e62d97f441d747f589829ba0f5b1261c",
       "IPY_MODEL_f0d4f7a5fc0244f28d1b07bfee8b9746",
       "IPY_MODEL_f9c01e2dce6a46b389ebe3b366658a98"
      ],
      "layout": "IPY_MODEL_e8a1f0b90f2d4ed1bf34b25fb70de489"
     }
    },
    "f0d4f7a5fc0244f28d1b07bfee8b9746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1a58901df8640798cdf4d97ba376b45",
      "max": 1433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0673d974ee8a44ec8978ba73aeade63b",
      "value": 33
     }
    },
    "f9c01e2dce6a46b389ebe3b366658a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39e01e2aa6704920864de0d34459bf9c",
      "placeholder": "​",
      "style": "IPY_MODEL_d16e87bc97664db8a9b1fa7a3fa8bdfe",
      "value": " 33/1433 [04:00&lt;2:41:44,  6.93s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
